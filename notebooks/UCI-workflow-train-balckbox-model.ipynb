{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2c3ffa8c-2a42-4d03-920b-bf276ab2ae83",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "98ee37ad-06fd-4b3b-a2d5-f79467354f3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 06:29:46.920751: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 06:29:50.401675: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer.so.7'; dlerror: libnvinfer.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jovyan/.conda/envs/env-3.8/lib/:/home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/nvidia/cudnn/lib:/home/jovyan/.conda/envs/env-3.8/lib/:/home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/nvidia/cudnn/lib:\n",
      "2025-04-09 06:29:50.401867: W tensorflow/compiler/xla/stream_executor/platform/default/dso_loader.cc:64] Could not load dynamic library 'libnvinfer_plugin.so.7'; dlerror: libnvinfer_plugin.so.7: cannot open shared object file: No such file or directory; LD_LIBRARY_PATH: /home/jovyan/.conda/envs/env-3.8/lib/:/home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/nvidia/cudnn/lib:/home/jovyan/.conda/envs/env-3.8/lib/:/home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/nvidia/cudnn/lib:\n",
      "2025-04-09 06:29:50.401877: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Cannot dlopen some TensorRT libraries. If you would like to use Nvidia GPU with TensorRT, please make sure the missing libraries mentioned above are installed properly.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from tensorflow.compat.v1.keras.backend import get_session\n",
    "import tensorflow as tf\n",
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"]=\"0\" # second gpu\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "gpu_to_limit = gpus[0]  # or 1, watch out for CUDA_VISIBLE_DEVICES\n",
    "gpu_mem_mb = 24564  # tyle mamy\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpu_to_limit, [tf.config.LogicalDeviceConfiguration(memory_limit=gpu_mem_mb//2)]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "5d07bec9-6e9a-4da6-8744-be13a282fb4f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "source": [
    "import tensorflow as tf\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "27728a52-c641-43e3-978a-7e4b9ad669e3",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from itertools import cycle\n",
    "from ruptures.utils import pairwise\n",
    "from sklearn.model_selection import train_test_split\n",
    "import graphviz\n",
    "\n",
    "from sklearn import tree\n",
    "from sklearn.tree import  export_graphviz\n",
    "from graphviz import Source\n",
    "from sklearn.preprocessing import StandardScaler,MinMaxScaler\n",
    "from IPython.display import display\n",
    "from IPython.display import SVG\n",
    "import shap\n",
    "import matplotlib.pyplot as plt\n",
    "import ruptures as rpt\n",
    "from tslearn.clustering import TimeSeriesKMeans\n",
    "from tslearn.utils import to_time_series_dataset\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from aeon.datasets import load_classification\n",
    "from windowshap import SlidingWindowSHAP, StationaryWindowSHAP, DynamicWindowSHAP\n",
    "import time\n",
    "from tslearn.clustering import KShape\n",
    "from kshape.core import KShapeClusteringCPU\n",
    "from kshape.core_gpu import KShapeClusteringGPU\n",
    "from tslearn.preprocessing import TimeSeriesScalerMeanVariance\n",
    "from sklearn.cluster import KMeans"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "226fe266-1bb1-412f-abba-2fa3a5378f9c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsproto\n",
    "from sklearn.metrics import f1_score, precision_score, recall_score, accuracy_score\n",
    "from scipy import stats \n",
    "import xgboost as xgb"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e5897d9-89ca-457d-b6bd-4f4ed1ac878d",
   "metadata": {},
   "source": [
    "# Create model architecture"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a94117ee-3b54-4f54-88ee-8b995b1979de",
   "metadata": {},
   "outputs": [],
   "source": [
    "# convlstm model\n",
    "from numpy import mean\n",
    "from numpy import std\n",
    "from numpy import dstack\n",
    "from pandas import read_csv\n",
    "from keras.models import Sequential\n",
    "from keras.layers import Dense\n",
    "from keras.layers import Flatten\n",
    "from keras.layers import Dropout\n",
    "from keras.layers import LSTM\n",
    "from keras.layers import Reshape\n",
    "from keras.layers import TimeDistributed\n",
    "from keras.layers import ConvLSTM2D,ConvLSTM1D\n",
    "from keras.utils import to_categorical\n",
    "from matplotlib import pyplot\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "import math\n",
    "\n",
    "def divisorGenerator(n):\n",
    "    large_divisors = []\n",
    "    for i in range(1, int(math.sqrt(n) + 1)):\n",
    "        if n % i == 0:\n",
    "            yield i\n",
    "            if i*i != n:\n",
    "                large_divisors.append(n / i)\n",
    "    for divisor in reversed(large_divisors):\n",
    "        yield divisor\n",
    " \n",
    "def load_dataset_aeon(dsname):\n",
    "    X, y, meta_data = load_classification(dsname)\n",
    "    X = np.moveaxis(X,1,2)\n",
    "    \n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    trainX, testX, trainy, testy= train_test_split(X,y)\n",
    "    \n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "def generate_label_weights(train_y):\n",
    "    # Count the occurrences of each class by summing across rows\n",
    "    class_counts = np.sum(train_y, axis=0)\n",
    "\n",
    "    # Calculate the total number of samples\n",
    "    total_samples = np.sum(class_counts)\n",
    "\n",
    "    # Calculate the weight for each class\n",
    "    class_weights = {}\n",
    "    num_classes = train_y.shape[1]\n",
    "    for i in range(num_classes):\n",
    "        class_weights[i] = total_samples / (num_classes * class_counts[i])\n",
    "\n",
    "    return class_weights\n",
    "\n",
    "def train_lstmconv_model(trainX, trainy, testX, testy, kernel=9):\n",
    "    # define model\n",
    "    verbose, epochs, batch_size = 0, 25, 64\n",
    "    n_timesteps, n_features, n_outputs = trainX.shape[1], trainX.shape[2], trainy.shape[1]\n",
    "    # reshape into subsequences (samples, time steps, rows, cols, channels)\n",
    "\n",
    "    dg = divisorGenerator(n_timesteps)\n",
    "    n_steps = int([next(dg) for i in range(3)][-1])\n",
    "    n_length = int(n_timesteps/n_steps)\n",
    "    # define model\n",
    "    print(f'Input shape (time, rows, channels): {(n_steps, n_length, n_features)}')\n",
    "    model = Sequential()\n",
    "    model.add(Reshape((n_steps, n_length, n_features), input_shape=trainX.shape[1:]))\n",
    "    model.add(ConvLSTM1D(filters=64, kernel_size=kernel, padding='same', activation='relu', strides=1, \n",
    "                         data_format='channels_last',\n",
    "                         return_sequences=True,\n",
    "                         input_shape=(n_steps, n_length, n_features)))\n",
    "    model.add(ConvLSTM1D(filters=32, kernel_size=kernel, padding='same', activation='relu', strides=1, \n",
    "                         data_format='channels_last',\n",
    "                         return_sequences=True,\n",
    "                         input_shape=(n_steps, n_length, n_features)))\n",
    "    model.add(Dropout(0.5))\n",
    "    model.add(Flatten(name='embedding')) #is this a latent representation we could use for clustering?\n",
    "    model.add(Dense(100, activation='relu'))\n",
    "    model.add(Dense(n_outputs, activation='softmax'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['accuracy'])\n",
    "    \n",
    "\n",
    "    print(model.summary())\n",
    "    # fit network\n",
    "    weights = generate_label_weights(trainy)\n",
    "    model.fit(trainX, trainy, epochs=epochs, batch_size=batch_size, verbose=verbose,class_weight=weights)\n",
    "    # evaluate model\n",
    "    _, accuracy = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    return accuracy, model\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    " \n",
    "\n",
    "# run an experiment\n",
    "def evaluate_lstmconv_model(dataset, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset_aeon(dataset)\n",
    "\n",
    "    #fill missing with zeros\n",
    "    trainX = np.nan_to_num(trainX, nan=0)\n",
    "    testX = np.nan_to_num(testX, nan=0)\n",
    "    \n",
    "    scaler = StandardScaler()\n",
    "    trainX = scaler.fit_transform(trainX.reshape(-1, trainX.shape[-1])).reshape(trainX.shape)\n",
    "    testX = scaler.transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score,model = train_lstmconv_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r+1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return model, trainX, trainy, testX,testy,score\n",
    " "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d050ffed-4f3a-4454-bd51-76c56d27626a",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "import os\n",
    "import keras\n",
    "def save_bundle(model, trainX,trainy,testX,testy,svtr,svts, dsname, dir='./results'):\n",
    "    if not os.path.isdir(f'{dir}/{dsname}'):\n",
    "        os.makedirs(f'{dir}/{dsname}')\n",
    "\n",
    "    model.save(f'{dir}/{dsname}/model.h5',save_format='h5')\n",
    "    pickle.dump(trainX, open(f'{dir}/{dsname}/trainX.pickle','wb'))\n",
    "    pickle.dump(trainy, open(f'{dir}/{dsname}/trainy.pickle','wb'))\n",
    "    pickle.dump(testX, open(f'{dir}/{dsname}/testX.pickle','wb'))\n",
    "    pickle.dump(testy, open(f'{dir}/{dsname}/testy.pickle','wb'))\n",
    "    pickle.dump(svtr, open(f'{dir}/{dsname}/svtr.pickle','wb'))\n",
    "    pickle.dump(svts, open(f'{dir}/{dsname}/svts.pickle','wb'))\n",
    "\n",
    "    \n",
    "def load_bundle(dsname, dir='./results'):\n",
    "    model = keras.models.load_model(f'{dir}/{dsname}/model.h5')\n",
    "    \n",
    "    trainX= pickle.load( open(f'{dir}/{dsname}/trainX.pickle','rb'))\n",
    "    trainy = pickle.load(open(f'{dir}/{dsname}/trainy.pickle','rb'))\n",
    "    testX= pickle.load(open(f'{dir}/{dsname}/testX.pickle','rb'))\n",
    "    testy= pickle.load(open(f'{dir}/{dsname}/testy.pickle','rb'))\n",
    "    svtr = pickle.load(open(f'{dir}/{dsname}/svtr.pickle','rb'))\n",
    "    svts = pickle.load(open(f'{dir}/{dsname}/svts.pickle','rb'))\n",
    "\n",
    "    return model, trainX,trainy,testX,testy,svtr,svts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "de3292f7-9906-4d12-b6a2-5b6423420271",
   "metadata": {},
   "source": [
    "## Multivariate model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "896f742a-6517-475c-99c2-126a46e442ba",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "\n",
    "bg_size = 1000\n",
    "window_len = 23\n",
    "stride =10\n",
    "shap_version = 'deep' #'window'\n",
    "absshap=True #Wheather to use absolut values for shap, or use sign of shapp in changepoint detection\n",
    "#limits due to memory limitations of GPUs\n",
    "SEQUENCE_LENGTH_LIMIT=2000\n",
    "DIMENSIONALITY_LIMIT=300\n",
    "ACC_LIMIT=0\n",
    "\n",
    "multivariate_ts_list = [\n",
    "    \"ArticularyWordRecognition\", \"AtrialFibrillation\", \"BasicMotions\", \"Cricket\", \"Epilepsy\",\n",
    "    \"ERing\", \"EthanolConcentration\", \"FaceDetection\", \"FingerMovements\", \"HandMovementDirection\",\n",
    "    \"Handwriting\", \"Heartbeat\", \"Libras\", \"LSST\", \"NATOPS\", \"PenDigits\", \"RacketSports\",\n",
    "    \"SelfRegulationSCP1\", \"SelfRegulationSCP2\", \"UWaveGestureLibrary\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "skipds=[]\n",
    "summary = []\n",
    "for p multivariate_ts_list:\n",
    "    print(f'Time for {p}')\n",
    "    if p in skipds:\n",
    "        print(f'Skipping: {p}')\n",
    "        continue\n",
    "    try:\n",
    "        X, y, meta_data = load_classification(p)\n",
    "        X = np.moveaxis(X,1,2)\n",
    "        if X.shape[1] > SEQUENCE_LENGTH_LIMIT:\n",
    "            print(f'Skipping: {p} due to sequence length')\n",
    "            summary.append([p,np.nan,'LENGTH'])\n",
    "            continue\n",
    "        if X.shape[2] > DIMENSIONALITY_LIMIT:\n",
    "            print(f'Skipping: {p} due to dimensionality size')\n",
    "            summary.append([p,np.nan,'DIMENSIONALITY'])\n",
    "            continue\n",
    "        \n",
    "        print(f'Classifying: {p}')\n",
    "        model, trainX, trainy, testX,testy,score=evaluate_lstmconv_model(p)\n",
    "        if score < ACC_LIMIT:\n",
    "            print(f'Skipping: {p} due to low score which is {score}')\n",
    "            skipds.append(p)\n",
    "            continue\n",
    "        print(f'Size of the problem: {trainX.shape}')\n",
    "        \n",
    "        if shap_version == 'window':\n",
    "            #There is a problem with Windowed version for more than two classes\n",
    "            indexes = np.arange(0, len(trainX))\n",
    "            np.random.shuffle(indexes)\n",
    "            maxid = min(bg_size, len(trainX))\n",
    "            background_data = trainX[indexes[:maxid]]\n",
    "            \n",
    "            sv_ts = np.zeros((len(testX),testX.shape[1], testX.shape[2]))\n",
    "            sv_tr = np.zeros((len(trainX),trainX.shape[1], trainX.shape[2]))\n",
    "            \n",
    "            for i in range(len(testX)):\n",
    "                gtw = SlidingWindowSHAP(model, stride, window_len, background_data, testX[i:i+1], model_type='lstm')\n",
    "                sv_ts[i,:,:] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "            for i in range(len(trainX)):\n",
    "                gtw = SlidingWindowSHAP(model, stride, window_len, background_data, trainX[i:i+1], model_type='lstm')\n",
    "                sv_tr[i,:,:] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "        elif shap_version == 'deep':\n",
    "            indexes = np.arange(0, len(trainX))\n",
    "            np.random.shuffle(indexes)\n",
    "            maxid = min(bg_size, len(trainX))\n",
    "            background_data = trainX[indexes[:maxid]]\n",
    "            \n",
    "            explainer = shap.DeepExplainer(model,background_data)\n",
    "            shap_values_ts = explainer.shap_values(testX, check_additivity=False)\n",
    "            shap_values_tr = explainer.shap_values(trainX, check_additivity=False)\n",
    "            if absshap:\n",
    "                sv_ts = abs(np.array(shap_values_ts)).mean(axis=0) # This basically returns the average importance over the feature/sample\n",
    "                                                   # Not taking into account the sign of shap value, as it is not required\n",
    "                                                   # for breakpoints calculation\n",
    "                sv_tr = abs(np.array(shap_values_tr)).mean(axis=0) # This basically returns the average importance over the feature/sample\n",
    "                                                       # Not taking into account the sign of shap value, as it is not required\n",
    "                                                       # for breakpoints calculation\n",
    "            else:\n",
    "                indexer = np.argmax(model.predict(testX), axis=1)\n",
    "                sv_ts=[]\n",
    "                for i in range(0,len(testX)):\n",
    "                    sv_ts.append([shap_values_ts[indexer[i]][i,:]])\n",
    "                sv_ts=np.concatenate(sv_ts)\n",
    "                indexer = np.argmax(model.predict(trainX), axis=1)\n",
    "                sv_tr=[]\n",
    "                for i in range(0,len(trainX)):\n",
    "                    sv_tr.append([shap_values_tr[indexer[i]][i,:]])\n",
    "                sv_tr=np.concatenate(sv_tr)\n",
    "\n",
    "        save_bundle(model, trainX,trainy,testX,testy,sv_tr,sv_ts, p,dir='./results/multivariate-models')\n",
    "        summary.append([p,score,'OK'])\n",
    "            \n",
    "        tf.keras.backend.clear_session()\n",
    "    except:\n",
    "        summary.append([p,np.nan,'Exception'])\n",
    "        print(f'Failed...')\n",
    "        skipds.append(p)\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "multi_model_summary = './results/multivariate-summary.csv'\n",
    "pd.DataFrame(summary,columns=['dataset','score','status']).to_csv(multi_model_summary,index=False)\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0582d40c-5c55-479e-a488-81bef34654e2",
   "metadata": {},
   "source": [
    "## Univariate model training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "26991023-1bc6-43d3-b2e4-cb2748cbbd97",
   "metadata": {},
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "bg_size = 1000\n",
    "window_len = 23\n",
    "stride =10\n",
    "shap_version = 'deep' #'window'\n",
    "#limits due to memory limitations of GPUs\n",
    "SEQUENCE_LENGTH_LIMIT=1000\n",
    "DIMENSIONALITY_LIMIT=200\n",
    "ACC_LIMIT=0\n",
    "absshap=True\n",
    "\n",
    "skipds = []\n",
    "summary=[]\n",
    "\n",
    "univariate_ts_list = [\n",
    "    \"Adiac\", \"Beef\", \"BeetleFly\", \"BirdChicken\", \"BME\", \"CBF\", \"Chinatown\", \"Coffee\", \"Computers\",\n",
    "    \"CricketX\", \"CricketY\", \"CricketZ\", \"Crop\", \"DiatomSizeReduction\", \"DistalPhalanxOutlineAgeGroup\",\n",
    "    \"DistalPhalanxOutlineCorrect\", \"DistalPhalanxTW\", \"DodgerLoopDay\", \"DodgerLoopGame\", \"DodgerLoopWeekend\",\n",
    "    \"Earthquakes\", \"ECG200\", \"ECG5000\", \"ECGFiveDays\", \"ElectricDevices\", \"FaceFour\", \"FiftyWords\", \"FordA\",\n",
    "    \"FordB\", \"FreezerRegularTrain\", \"FreezerSmallTrain\", \"Fungi\", \"GunPoint\", \"GunPointAgeSpan\",\n",
    "    \"GunPointMaleVersusFemale\", \"GunPointOldVersusYoung\", \"Herring\", \"InsectWingbeatSound\",\n",
    "    \"ItalyPowerDemand\", \"LargeKitchenAppliances\", \"Lightning2\", \"Lightning7\", \"Meat\", \"MedicalImages\",\n",
    "    \"MiddlePhalanxOutlineAgeGroup\", \"MiddlePhalanxOutlineCorrect\", \"MiddlePhalanxTW\", \"MoteStrain\", \"OliveOil\",\n",
    "    \"OSULeaf\", \"PhalangesOutlinesCorrect\", \"Plane\", \"PowerCons\", \"ProximalPhalanxOutlineAgeGroup\",\n",
    "    \"ProximalPhalanxOutlineCorrect\", \"ProximalPhalanxTW\", \"RefrigerationDevices\", \"ScreenType\", \"ShapeletSim\",\n",
    "    \"ShapesAll\", \"SmallKitchenAppliances\", \"SmoothSubspace\", \"SonyAIBORobotSurface1\", \"SonyAIBORobotSurface2\",\n",
    "    \"Strawberry\", \"SwedishLeaf\", \"Symbols\", \"SyntheticControl\", \"ToeSegmentation2\", \"Trace\", \"TwoLeadECG\",\n",
    "    \"TwoPatterns\", \"UMD\", \"UWaveGestureLibraryAll\", \"UWaveGestureLibraryX\", \"UWaveGestureLibraryY\",\n",
    "    \"UWaveGestureLibraryZ\", \"Wafer\", \"Wine\", \"WordSynonyms\", \"Worms\", \"WormsTwoClass\", \"Yoga\"\n",
    "]\n",
    "\n",
    "\n",
    "\n",
    "for p in univariate_ts_list:\n",
    "\n",
    "    if p == continue_after:\n",
    "        continue_after=None\n",
    "        continue\n",
    "    elif continue_after is not None:\n",
    "        continue\n",
    "    \n",
    "    print(f'Time for {p}')\n",
    "    if p in skipds:\n",
    "        print(f'Skipping: {p}')\n",
    "        continue\n",
    "    try:\n",
    "        X, y, meta_data = load_classification(p)\n",
    "        X = np.moveaxis(X,1,2)\n",
    "        if X.shape[1] > SEQUENCE_LENGTH_LIMIT:\n",
    "            print(f'Skipping: {p} due to sequence length')\n",
    "            summary.append([p,np.nan,'LENGTH'])\n",
    "            continue\n",
    "        if X.shape[2] > DIMENSIONALITY_LIMIT:\n",
    "            print(f'Skipping: {p} due to dimensionality size')\n",
    "            summary.append([p,np.nan,'DIMENSIONALITY'])\n",
    "            continue\n",
    "\n",
    "        print(f'Classifying: {p}')\n",
    "        model, trainX, trainy, testX,testy,score=evaluate_lstmconv_model(p)\n",
    "        if score < ACC_LIMIT:\n",
    "            print(f'Skipping: {p} due to low score which is {score}')\n",
    "            skipds.append(p)\n",
    "            continue\n",
    "        print(f'Size of the problem: {trainX.shape}')\n",
    "        if shap_version == 'window':\n",
    "            #There is a problem with Windowed version for more than two classes\n",
    "            indexes = np.arange(0, len(trainX))\n",
    "            np.random.shuffle(indexes)\n",
    "            maxid = min(bg_size, len(trainX))\n",
    "            background_data = trainX[indexes[:maxid]]\n",
    "            \n",
    "            sv_ts = np.zeros((len(testX),testX.shape[1], testX.shape[2]))\n",
    "            sv_tr = np.zeros((len(trainX),trainX.shape[1], trainX.shape[2]))\n",
    "            \n",
    "            for i in range(len(testX)):\n",
    "                gtw = SlidingWindowSHAP(model, stride, window_len, background_data, testX[i:i+1], model_type='lstm')\n",
    "                sv_ts[i,:,:] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "            for i in range(len(trainX)):\n",
    "                gtw = SlidingWindowSHAP(model, stride, window_len, background_data, trainX[i:i+1], model_type='lstm')\n",
    "                sv_tr[i,:,:] = gtw.shap_values(num_output=trainy.shape[1])\n",
    "        elif shap_version == 'deep':\n",
    "            indexes = np.arange(0, len(trainX))\n",
    "            np.random.shuffle(indexes)\n",
    "            maxid = min(bg_size, len(trainX))\n",
    "            background_data = trainX[indexes[:maxid]]\n",
    "            \n",
    "            explainer = shap.DeepExplainer(model,background_data)\n",
    "            shap_values_ts = explainer.shap_values(testX, check_additivity=False)\n",
    "            shap_values_tr = explainer.shap_values(trainX, check_additivity=False)\n",
    "            if absshap:\n",
    "                sv_ts = abs(np.array(shap_values_ts)).mean(axis=0) # This basically returns the average importance over the feature/sample\n",
    "                                                   # Not taking into account the sign of shap value, as it is not required\n",
    "                                                   # for breakpoints calculation\n",
    "                sv_tr = abs(np.array(shap_values_tr)).mean(axis=0) # This basically returns the average importance over the feature/sample\n",
    "                                                       # Not taking into account the sign of shap value, as it is not required\n",
    "                                                       # for breakpoints calculation\n",
    "            else:\n",
    "                indexer = np.argmax(model.predict(testX), axis=1)\n",
    "                sv_ts=[]\n",
    "                for i in range(0,len(testX)):\n",
    "                    sv_ts.append([shap_values_ts[indexer[i]][i,:]])\n",
    "                sv_ts=np.concatenate(sv_ts)\n",
    "                indexer = np.argmax(model.predict(trainX), axis=1)\n",
    "                sv_tr=[]\n",
    "                for i in range(0,len(trainX)):\n",
    "                    sv_tr.append([shap_values_tr[indexer[i]][i,:]])\n",
    "                sv_tr=np.concatenate(sv_tr)\n",
    "                \n",
    "        save_bundle(model, trainX,trainy,testX,testy,sv_tr,sv_ts, p,dir='./results/univariat-models')\n",
    "        \n",
    "        summary.append([p,score,'OK'])\n",
    "            \n",
    "        tf.keras.backend.clear_session()\n",
    "    except:\n",
    "        summary.append([p,np.nan,'Exception'])\n",
    "        print(f'Failed...')\n",
    "        skipds.append(p)\n",
    "        tf.keras.backend.clear_session()\n",
    "\n",
    "uni_model_summary = './results/univariate-summary.csv'\n",
    "pd.DataFrame(summary,columns=['dataset','score','status']).to_csv(uni_model_summary,index=False)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f34a38e-e540-44e2-88b1-b9f341048c56",
   "metadata": {},
   "source": [
    "## How to load pretrained model?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c42210c3-308d-4538-a8de-67896fec321d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling GlorotUniform.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Orthogonal.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n",
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/env-3.8/lib/python3.8/site-packages/tensorflow/python/ops/init_ops.py:93: calling Zeros.__init__ (from tensorflow.python.ops.init_ops) with dtype is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Call initializer instance with the dtype argument instead of passing it to the constructor\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-04-09 06:30:26.883190: I tensorflow/core/platform/cpu_feature_guard.cc:193] This TensorFlow binary is optimized with oneAPI Deep Neural Network Library (oneDNN) to use the following CPU instructions in performance-critical operations:  AVX2 FMA\n",
      "To enable them in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-04-09 06:30:27.148368: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1613] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12282 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2025-04-09 06:30:27.260125: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:357] MLIR V1 optimization pass is not enabled\n",
      "2025-04-09 06:30:28.065957: W tensorflow/c/c_api.cc:291] Operation '{name:'dense/bias/Assign' id:665 op device:{requested: '', assigned: ''} def:{{{node dense/bias/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense/bias, dense/bias/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-04-09 06:30:28.689707: W tensorflow/c/c_api.cc:291] Operation '{name:'dense/kernel/v/Assign' id:940 op device:{requested: '', assigned: ''} def:{{{node dense/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense/kernel/v, dense/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    }
   ],
   "source": [
    "model, trainXo,trainyo,testX,testy,trainsvo,testsv = load_bundle('Crop',dir='./results/univariate-models')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "ee11733d-6d44-4565-a43a-372cba8b013b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 23, 2, 1)          0         \n",
      "                                                                 \n",
      " conv_lstm1d (ConvLSTM1D)    (None, 23, 2, 64)         150016    \n",
      "                                                                 \n",
      " conv_lstm1d_1 (ConvLSTM1D)  (None, 23, 2, 32)         110720    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 23, 2, 32)         0         \n",
      "                                                                 \n",
      " embedding (Flatten)         (None, 1472)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               147300    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 24)                2424      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 410,460\n",
      "Trainable params: 410,460\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2e590d54-22c9-46d1-9fa9-a8a0f53ff0ca",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8 env",
   "language": "python",
   "name": "env-3.8"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

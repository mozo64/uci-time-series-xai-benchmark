{
 "cells": [
  {
   "metadata": {},
   "cell_type": "code",
   "source": "! wget https://raw.githubusercontent.com/vsubbian/WindowSHAP/main/windowshap.py -P utils/",
   "id": "2d5455d1f0d57e56",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# conda activate uci_38\n",
    "# conda install -c conda-forge aeon\n",
    "# conda install shap lime\n",
    "# pip install windowshap --no-deps\n",
    "# pip install anchor-exp --no-deps\n",
    "# conda install flask"
   ],
   "id": "c1bc5f888e65c471",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:06:07.392644Z",
     "start_time": "2025-03-18T11:06:07.372085Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "# KERNEL = eb0c..."
   ],
   "id": "f639611a350d3b1d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:06:15.807989Z",
     "start_time": "2025-03-18T11:06:15.441580Z"
    }
   },
   "cell_type": "code",
   "source": "ls",
   "id": "f29eaa35a6033885",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auxiliary_mushrooms.py             \u001B[0m\u001B[01;34mmushroom-serialised\u001B[0m/\r\n",
      "duplicity-png.txt                  pkl_luiz.pkl\r\n",
      "environment.yml                    \u001B[01;34m__pycache__\u001B[0m/\r\n",
      "\u001B[01;34mextracted_data\u001B[0m/                    \u001B[01;36mREADME.txt\u001B[0m@\r\n",
      "\u001B[01;34mfast\u001B[0m/                              \u001B[01;34mresults\u001B[0m/\r\n",
      "formatted_table_image.png          \u001B[01;32mres_usage.sh\u001B[0m*\r\n",
      "\u001B[01;34mintel\u001B[0m/                             shap_values_X_val_cache.pkl\r\n",
      "\u001B[01;34mlogs\u001B[0m/                              \u001B[01;34mshared\u001B[0m/\r\n",
      "\u001B[01;34mmodels\u001B[0m/                            \u001B[01;34mslow\u001B[0m/\r\n",
      "mtm_ecg_results_v1_with_split.csv  \u001B[01;34mutils\u001B[0m/\r\n",
      "\u001B[01;34mMushroomDataset\u001B[0m/                   \u001B[01;34mXAI-ISI\u001B[0m/\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:06:23.319215Z",
     "start_time": "2025-03-18T11:06:19.162782Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"-1\"  # 0 - first gpu, 1 - second, \"0,1\" - both gpu, first used\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "np.object = object\n",
    "np.int = int\n",
    "np.float = float\n",
    "np.bool = bool\n",
    "np.typeDict = np.sctypeDict\n",
    "\n",
    "import gc\n",
    "import multiprocessing\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import requests\n",
    "import shap\n",
    "import tensorflow as tf\n",
    "import tensorflow.keras as keras  # import keras\n",
    "import warnings\n",
    "import sys\n",
    "import time\n",
    "import random\n",
    "import tempfile\n",
    "import string\n",
    "import traceback\n",
    "import json\n",
    "import alibi\n",
    "\n",
    "from anchor import anchor_tabular  # pip install anchor-exp --no-deps\n",
    "from alibi.explainers import AnchorTabular\n",
    "from datetime import datetime\n",
    "from aeon.datasets import load_classification\n",
    "from lime import lime_tabular\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder\n",
    "from tensorflow.keras import backend as K\n",
    "# from tensorflow.keras.layers import ConvLSTM1D  # only for h5 and does not work with new tf\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.keras.initializers import glorot_uniform  # only for h5 \n",
    "from tensorflow.python.util import deprecation\n",
    "from typing import Any, Dict, List, Optional, Set, Tuple\n",
    "from utils.windowshap import SlidingWindowSHAP\n",
    "from multiprocessing import Queue, Lock\n",
    "from concurrent.futures import ProcessPoolExecutor, TimeoutError as FuturesTimeoutError\n",
    "\n",
    "print(tf.__version__)  # print(tf.keras.__version__)\n",
    "print(alibi.explainers.__all__)\n",
    "\n",
    "tempfile.tempdir = \"fast/tmp\"\n",
    "os.makedirs(tempfile.tempdir, exist_ok=True)"
   ],
   "id": "817f63ac03c18d12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "2025-03-18 11:06:20.436981: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2025-03-18 11:06:21.285289: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n",
      "['ALE', 'AnchorTabular', 'DistributedAnchorTabular', 'AnchorText', 'AnchorImage', 'CEM', 'Counterfactual', 'CounterfactualProto', 'CounterfactualRL', 'CounterfactualRLTabular', 'plot_ale', 'IntegratedGradients', 'KernelShap', 'TreeShap', 'GradientSimilarity']\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:06:23.912487Z",
     "start_time": "2025-03-18T11:06:23.801067Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "ops.logging.set_verbosity(ops.logging.ERROR)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(gpus)\n",
    "if gpus:\n",
    "    gpu_to_limit = gpus[0]  # or 1, watch out for CUDA_VISIBLE_DEVICES\n",
    "    print(gpu_to_limit)"
   ],
   "id": "22334058a3a1fe88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/uci_38v4/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[]\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 11:06:23.909467: E tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:268] failed call to cuInit: CUDA_ERROR_NO_DEVICE: no CUDA-capable device is detected\n",
      "2025-03-18 11:06:23.909495: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:168] retrieving CUDA diagnostic information for host: ba97f2f42a1e\n",
      "2025-03-18 11:06:23.909501: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:175] hostname: ba97f2f42a1e\n",
      "2025-03-18 11:06:23.909695: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:199] libcuda reported version is: 550.90.7\n",
      "2025-03-18 11:06:23.909716: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:203] kernel reported version is: 550.90.7\n",
      "2025-03-18 11:06:23.909722: I tensorflow/compiler/xla/stream_executor/cuda/cuda_diagnostics.cc:309] kernel version seems to match DSO: 550.90.7\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "if gpus:\n",
    "    gpu_mem_mb = 24564  # tyle mamy\n",
    "    tf.config.set_logical_device_configuration(\n",
    "        gpu_to_limit, [tf.config.LogicalDeviceConfiguration(memory_limit=gpu_mem_mb // 10)]\n",
    "    )\n",
    "\n",
    "    print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "data_path = \"shared/UCI-Benchmark/\"\n",
    "batch_size = 64"
   ],
   "id": "8dde39802239d214",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:16:06.491994Z",
     "start_time": "2025-03-18T11:16:06.432161Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# MODEL_MANAGER = \"http://192.168.48.1:5005\"\n",
    "# MODEL_MANAGER = \"http://172.16.216.132:5005\"\n",
    "MODEL_MANAGER = \"http://172.16.216.132:6005\""
   ],
   "id": "98ee37ad-06fd-4b3b-a2d5-f79467354f3d",
   "outputs": [],
   "execution_count": 21
  },
  {
   "cell_type": "markdown",
   "id": "4e5897d9-89ca-457d-b6bd-4f4ed1ac878d",
   "metadata": {},
   "source": [
    "# Real Data example"
   ]
  },
  {
   "cell_type": "code",
   "id": "a94117ee-3b54-4f54-88ee-8b995b1979de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:18.377750Z",
     "start_time": "2025-03-18T11:07:18.318612Z"
    }
   },
   "source": [
    "# convlstm model\n",
    "def load_dataset_aeon(dsname):\n",
    "    X, y, meta_data = load_classification(dsname)\n",
    "    X = np.moveaxis(X, 1, 2)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    trainX, testX, trainy, testy = train_test_split(X, y)\n",
    "\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "\n",
    "# run an experiment\n",
    "def train_model(trainX, trainy, testX, testy):\n",
    "    return 1.0, keras.models.Sequential([])\n",
    "\n",
    "\n",
    "def train_lstmconv_model(trainX, trainy, testX, testy):\n",
    "    return 1.0, keras.models.Sequential([])\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:19.127888Z",
     "start_time": "2025-03-18T11:07:19.061116Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# @tf.function\n",
    "def evaluate_model(model, testX, testy, batch_size, steps):\n",
    "    # f = io.StringIO()\n",
    "    # with redirect_stdout(f), redirect_stderr(f):\n",
    "    # sys.stderr = open(os.devnull, 'w')\n",
    "    if steps is not None:\n",
    "        result = model.evaluate(testX, testy, batch_size=batch_size, verbose=0, steps=steps)\n",
    "    else:\n",
    "        result = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    # sys.stderr = sys.__stderr__\n",
    "    return result"
   ],
   "id": "81d98f04cd5c1143",
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## save & load bundle",
   "id": "501b87308df64f8c"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:21.523298Z",
     "start_time": "2025-03-18T11:07:21.447647Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def save_bundle(model, trainX, trainy, testX, testy, svtr, svts, dsname,\n",
    "                dir='./results', explain_prefix=\"sv\", only_explain=False):\n",
    "    if not os.path.isdir(f'{dir}/{dsname}'):\n",
    "        os.makedirs(f'{dir}/{dsname}')\n",
    "\n",
    "    if not only_explain:\n",
    "        # model.save(f'{dir}/{dsname}/model.h5', save_format='h5')\n",
    "        model.save(f'{dir}/{dsname}/model_tf', save_format='tf')\n",
    "\n",
    "        pickle.dump(trainX, open(f'{dir}/{dsname}/trainX.pickle', 'wb'))\n",
    "        pickle.dump(trainy, open(f'{dir}/{dsname}/trainy.pickle', 'wb'))\n",
    "        pickle.dump(testX, open(f'{dir}/{dsname}/testX.pickle', 'wb'))\n",
    "        pickle.dump(testy, open(f'{dir}/{dsname}/testy.pickle', 'wb'))\n",
    "\n",
    "    if svtr is not None:\n",
    "        pickle.dump(svtr, open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'wb'))\n",
    "    if svts is not None:\n",
    "        pickle.dump(svts, open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'wb'))\n",
    "\n",
    "\n",
    "def load_bundle(dsname, dir='./results', explain_prefix=\"sv\", only_explain=False, load_model=True):\n",
    "    model, model_graph, trainX, trainy, testX, testy = None, None, None, None, None, None,\n",
    "    if not only_explain:\n",
    "        tf_model_path = f'{dir}/{dsname}/model_tf/1'\n",
    "        h5_model_path = f'{dir}/{dsname}/model.h5'\n",
    "\n",
    "        # TODO model_graph does not help in multiprocessing, but when once created, must be used, so I don't use it!\n",
    "        # model_graph = tf.Graph()\n",
    "        # with model_graph.as_default():\n",
    "        if load_model:\n",
    "            if os.path.exists(tf_model_path):\n",
    "                print(\"Loading model in tf format...\")\n",
    "                model = tf.keras.models.load_model(tf_model_path)\n",
    "            elif os.path.exists(h5_model_path):\n",
    "                print(\"Loading model in h5 format...\")\n",
    "                model = tf.keras.models.load_model(\n",
    "                    h5_model_path, custom_objects={'GlorotUniform': glorot_uniform, 'ConvLSTM1D': ConvLSTM1D})\n",
    "                # Save model in tf format if loaded from h5\n",
    "                if not os.path.exists(tf_model_path):\n",
    "                    os.makedirs(tf_model_path, exist_ok=True)\n",
    "                    model.save(tf_model_path, save_format='tf')\n",
    "                    print(f\"Model {dsname} saved in TensorFlow format at {tf_model_path}\")\n",
    "            else:\n",
    "                raise FileNotFoundError(\n",
    "                    f'Nie znaleziono modelu w formatach SavedModel ani H5 dla {dsname} w katalogu {dir}.')\n",
    "            # print(model.summary())\n",
    "\n",
    "        trainX = pickle.load(open(f'{dir}/{dsname}/trainX.pickle', 'rb'))\n",
    "        trainy = pickle.load(open(f'{dir}/{dsname}/trainy.pickle', 'rb'))\n",
    "        testX = pickle.load(open(f'{dir}/{dsname}/testX.pickle', 'rb'))\n",
    "        testy = pickle.load(open(f'{dir}/{dsname}/testy.pickle', 'rb'))\n",
    "\n",
    "    svtr = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'rb'))\n",
    "    svts = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'rb'))\n",
    "\n",
    "    return model, model_graph, trainX, trainy, testX, testy, svtr, svts"
   ],
   "id": "d050ffed-4f3a-4454-bd51-76c56d27626a",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "f885998e-9e52-4d0f-9398-df285e718c7f",
   "metadata": {},
   "source": "## Univariate"
  },
  {
   "cell_type": "code",
   "id": "5adfe00ef00318a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:23.817104Z",
     "start_time": "2025-03-18T11:07:23.760558Z"
    }
   },
   "source": [
    "#list directoies in  folder\n",
    "datasets_uni = [f.name for f in os.scandir(data_path + 'ds/univariate') if f.is_dir() and not f.name.startswith('.')]\n",
    "print(datasets_uni)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DodgerLoopGame', 'ProximalPhalanxTW', 'DodgerLoopDay', 'ECGFiveDays', 'UMD', 'Plane', 'ECG200', 'TwoPatterns', 'GunPointAgeSpan', 'DiatomSizeReduction', 'UWaveGestureLibraryAll', 'MedicalImages', 'Meat', 'Trace', 'MiddlePhalanxOutlineAgeGroup', 'Chinatown', 'DistalPhalanxOutlineAgeGroup', 'WormsTwoClass', 'DistalPhalanxOutlineCorrect', 'Strawberry', 'OliveOil', 'UWaveGestureLibraryX', 'SmoothSubspace', 'Fungi', 'ElectricDevices', 'SwedishLeaf', 'CricketX', 'ECG5000', 'PhalangesOutlinesCorrect', 'FaceFour', 'SyntheticControl', 'FordA', 'PowerCons', 'BeetleFly', 'GunPointMaleVersusFemale', 'Yoga', 'Herring', 'Crop', 'RefrigerationDevices', 'Worms', 'OSULeaf', 'ItalyPowerDemand', 'GunPoint', 'CBF', 'Symbols', 'ToeSegmentation2', 'TwoLeadECG', 'SmallKitchenAppliances', 'ShapesAll', 'ScreenType', 'Computers', 'CricketY', 'SonyAIBORobotSurface2', 'Wafer', 'MiddlePhalanxOutlineCorrect', 'Lightning2', 'UWaveGestureLibraryY', 'Wine', 'ProximalPhalanxOutlineCorrect', 'ShapeletSim', 'CricketZ', 'BME', 'Beef', 'DodgerLoopWeekend', 'UWaveGestureLibraryZ', 'ProximalPhalanxOutlineAgeGroup', 'LargeKitchenAppliances', 'FordB', 'FreezerRegularTrain', 'Lightning7', 'BirdChicken', 'GunPointOldVersusYoung', 'Coffee', 'FreezerSmallTrain', 'SonyAIBORobotSurface1', 'Adiac', 'MiddlePhalanxTW', 'Earthquakes', 'DistalPhalanxTW', 'FiftyWords', 'WordSynonyms', 'MoteStrain', 'ChlorineConcentration', 'InsectWingbeatSound']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:27.624740Z",
     "start_time": "2025-03-18T11:07:25.235565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in datasets_uni:\n",
    "    print(f'Loading... {p}')\n",
    "    ##trainsvo is shap calculated for train, testsv is shap calculated for test (NOTE: these are absolute values of shap)\n",
    "    model, _, trainXo, trainyo, testX, testy, trainsvo, testsv = load_bundle(p,\n",
    "                                                                             dir=data_path + 'ds/univariate')\n",
    "    # with HiddenPrints():\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size=batch_size, steps=None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    K.clear_session()\n",
    "    del model, trainXo, trainyo, testX, testy, trainsvo, testsv\n",
    "    gc.collect()\n",
    "\n",
    "    break\n"
   ],
   "id": "d510e5f8-96cb-49e7-8830-b6c389836845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... DodgerLoopGame\n",
      "Loading model in tf format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 11:07:26.934475: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2025-03-18 11:07:26.982976: W tensorflow/c/c_api.cc:304] Operation '{name:'AssignVariableOp_13' id:281 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node AssignVariableOp_13}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](dense_1/kernel/v, Identity_13)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-18 11:07:27.130845: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/mul' id:986 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy: 87.500\n"
     ]
    }
   ],
   "execution_count": 10
  },
  {
   "cell_type": "markdown",
   "id": "e7b57e83-6cce-4893-88b6-a9a905e486d9",
   "metadata": {},
   "source": "## Multivariate"
  },
  {
   "cell_type": "code",
   "id": "4a669e7be07298e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:33.142403Z",
     "start_time": "2025-03-18T11:07:33.083187Z"
    }
   },
   "source": [
    "#list directoies in  folder\n",
    "datasets_multi = [f.name for f in os.scandir(data_path + '/ds/multivariate') if\n",
    "                  f.is_dir() and not f.name.startswith('.')]\n",
    "print(datasets_multi)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArticularyWordRecognition', 'Handwriting', 'BasicMotions', 'HandMovementDirection', 'Heartbeat', 'FaceDetection', 'SelfRegulationSCP2', 'NATOPS', 'ERing', 'EthanolConcentration', 'FingerMovements', 'Epilepsy', 'Libras', 'AtrialFibrillation', 'PenDigits', 'Cricket', 'LSST', 'UWaveGestureLibrary', 'RacketSports', 'SelfRegulationSCP1']\n"
     ]
    }
   ],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:35.755117Z",
     "start_time": "2025-03-18T11:07:33.956421Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in datasets_multi:\n",
    "    print(f'Loading... {p}')\n",
    "    model, _, trainXo, trainyo, testX, testy, trainsvo, testsv = load_bundle(p,\n",
    "                                                                             dir=data_path + 'ds/multivariate')\n",
    "    # steps = len(testX) // batch_size\n",
    "    # if len(testX) % batch_size != 0:\n",
    "    #     steps += 1\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size, None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    # K.clear_session()\n",
    "    # del model, trainXo, trainyo, testX, testy, trainsvo, testsv\n",
    "    # gc.collect()\n",
    "\n",
    "    break"
   ],
   "id": "ea9d98d95e90affe",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... ArticularyWordRecognition\n",
      "Loading model in tf format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 11:07:35.411988: W tensorflow/c/c_api.cc:304] Operation '{name:'AssignVariableOp_20' id:301 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node AssignVariableOp_20}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](conv_lstm1d/kernel/m, Identity_20)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2025-03-18 11:07:35.551316: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/mul' id:992 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy: 96.528\n"
     ]
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:38.600426Z",
     "start_time": "2025-03-18T11:07:38.532517Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir(model)\n",
    "model.summary()"
   ],
   "id": "f189c686729b3294",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 3, 48, 9)          0         \n",
      "                                                                 \n",
      " conv_lstm1d (ConvLSTM1D)    (None, 3, 48, 64)         168448    \n",
      "                                                                 \n",
      " conv_lstm1d_1 (ConvLSTM1D)  (None, 3, 48, 32)         110720    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 48, 32)         0         \n",
      "                                                                 \n",
      " embedding (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               460900    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                2525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742593 (2.83 MB)\n",
      "Trainable params: 742593 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:40.260926Z",
     "start_time": "2025-03-18T11:07:40.204842Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainXo.shape)\n",
    "print(trainsvo.shape)"
   ],
   "id": "948d4e1474b94303",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431, 144, 9)\n",
      "(431, 144, 9)\n"
     ]
    }
   ],
   "execution_count": 14
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Sample",
   "id": "cec384b407732a0e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:53:10.814712Z",
     "start_time": "2024-11-11T13:53:10.772500Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# nvidia-smi\n",
    "# htop\n",
    "\n",
    "# ps aux | grep model_server_gpu\n",
    "#                                   pkill -f model_server_gpu\n",
    "\n",
    "# ports=$(ps aux | grep model_server | grep -- \"--port\" | awk -F '--port ' '{print $2}' | awk '{print $1}' | sort -n); echo \"Liczba portów: $(echo \"$ports\" | wc -l), Najniższy port: $(echo \"$ports\" | head -n 1), Najwyższy port: $(echo \"$ports\" | tail -n 1)\"\n",
    "\n",
    "# ps aux | grep model_server | grep -- \"--port\" | awk -F '--port ' '{print $2}' | awk '{print $1}' | sort -n | tr '\\n' ' '"
   ],
   "id": "938f45ba955ac666",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-12T09:09:41.437279Z",
     "start_time": "2024-11-12T09:09:41.350604Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tf.keras.backend.clear_session()\n",
    "# gc.collect()\n",
    "\n",
    "# directory, timeseries = \"univariate\", \"DodgerLoopGame\"\n",
    "# directory, timeseries = \"univariate\", \"Wafer\"\n",
    "# directory, timeseries = \"univariate\", \"Crop\"\n",
    "directory, timeseries = \"multivariate\", \"SelfRegulationSCP1\"\n",
    "# directory, timeseries = \"multivariate\", \"ArticularyWordRecognition\"\n",
    "# directory, timeseries = \"multivariate\", \"Heartbeat\"\n",
    "_, _, trainX_new, trainy_new, testX_new, testy_new, _, _ = load_bundle(\n",
    "    timeseries, dir=os.path.join(data_path, 'ds', directory), load_model=False)\n",
    "\n",
    "n_timesteps_new, n_features_new = trainX_new.shape[1], trainX_new.shape[2]\n",
    "print(n_timesteps_new, n_features_new)\n",
    "\n",
    "trainX_reshaped_new = trainX_new.reshape(-1, n_timesteps_new * n_features_new)\n",
    "index_new = 108\n",
    "instance_new = trainX_reshaped_new[index_new].reshape(-1, n_timesteps_new, n_features_new)\n",
    "\n",
    "print(instance_new.shape)\n",
    "print(instance_new[:, :10, :])\n",
    "type(instance_new)"
   ],
   "id": "f00e4d63571c79f7",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "896 6\n",
      "(1, 896, 6)\n",
      "[[[-0.15586902 -0.25176516  0.02883643 -0.04777336 -0.0622212\n",
      "   -0.2121953 ]\n",
      "  [-0.2401376  -0.19744996  0.06404643 -0.06184454 -0.06966394\n",
      "   -0.20806369]\n",
      "  [-0.29742364 -0.09628271  0.0847582  -0.08129587 -0.09323261\n",
      "   -0.12708425]\n",
      "  [-0.36218176 -0.0626985   0.06653184 -0.06184454 -0.1473992\n",
      "   -0.04817061]\n",
      "  [-0.40908989  0.00737225  0.06528914 -0.03080518 -0.19908488\n",
      "    0.03446147]\n",
      "  [-0.38708874 -0.0083833   0.00688196 -0.04363478 -0.21727824\n",
      "    0.12370412]\n",
      "  [-0.25840272  0.02271319 -0.05401064 -0.06846627 -0.2185187\n",
      "    0.15469115]\n",
      "  [-0.20402249  0.05795588 -0.06809464 -0.07591571 -0.21438384\n",
      "    0.15179903]\n",
      "  [-0.10273414  0.07868687 -0.0875637  -0.09950562 -0.1333407\n",
      "    0.21914417]\n",
      "  [-0.06910973  0.0604436  -0.06809464 -0.15372104 -0.05436498\n",
      "    0.22575474]]]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "numpy.ndarray"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explainers",
   "id": "8c78d0713c78f09f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:44.773752Z",
     "start_time": "2025-03-18T11:07:44.692643Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path, header=None, sep='|')\n",
    "    df = df[0].str.split(',', expand=True).dropna(axis=1)\n",
    "    df.columns = df.iloc[0]\n",
    "    return df[1:]\n",
    "\n",
    "\n",
    "def load_background_data(trainX: np.ndarray, bg_size: int) -> np.ndarray:\n",
    "    indexes = np.arange(len(trainX))\n",
    "    np.random.shuffle(indexes)\n",
    "    maxid = min(bg_size, len(trainX))\n",
    "    return trainX[indexes[:maxid]]\n",
    "\n",
    "\n",
    "def compute_window_shap(model: Any, trainX: np.ndarray, testX: np.ndarray, window_len: int, stride: int,\n",
    "                        bg_data: np.ndarray, output_shape: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sv_ts = np.zeros((len(testX), testX.shape[1], testX.shape[2]))\n",
    "    sv_tr = np.zeros((len(trainX), trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "    for i in range(len(testX)):\n",
    "        gtw = SlidingWindowSHAP(model, stride, window_len, bg_data, testX[i:i + 1], model_type='lstm')\n",
    "        sv_ts[i, :, :] = gtw.shap_values(num_output=output_shape)\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "        gtw = SlidingWindowSHAP(model, stride, window_len, bg_data, trainX[i:i + 1], model_type='lstm')\n",
    "        sv_tr[i, :, :] = gtw.shap_values(num_output=output_shape)\n",
    "\n",
    "    return sv_tr, sv_ts\n",
    "\n",
    "\n",
    "def compute_deep_shap(model: Any, trainX: np.ndarray, testX: np.ndarray, bg_data: np.ndarray, absshap: bool) -> Tuple[\n",
    "    np.ndarray, np.ndarray]:\n",
    "    explainer = shap.DeepExplainer(model, bg_data)\n",
    "    shap_values_ts = explainer.shap_values(testX, check_additivity=False)\n",
    "    shap_values_tr = explainer.shap_values(trainX, check_additivity=False)\n",
    "\n",
    "    if absshap:\n",
    "        return abs(np.array(shap_values_tr)).mean(axis=0), abs(np.array(shap_values_ts)).mean(axis=0)\n",
    "\n",
    "    indexer_ts = np.argmax(model.predict(testX), axis=1)\n",
    "    sv_ts = np.concatenate([shap_values_ts[indexer_ts[i]][i, :] for i in range(len(testX))])\n",
    "\n",
    "    indexer_tr = np.argmax(model.predict(trainX), axis=1)\n",
    "    sv_tr = np.concatenate([shap_values_tr[indexer_tr[i]][i, :] for i in range(len(trainX))])\n",
    "\n",
    "    return sv_tr, sv_ts\n",
    "\n",
    "\n",
    "def compute_deep_lime(model: Any, trainX: np.ndarray, testX: np.ndarray) -> Tuple[\n",
    "    np.ndarray, np.ndarray]:\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "\n",
    "    trainX_reshaped = trainX.reshape(-1, n_timesteps * n_features)\n",
    "    testX_reshaped = testX.reshape(-1, n_timesteps * n_features)\n",
    "\n",
    "    nan_count_train_X = np.isnan(trainX_reshaped).sum()\n",
    "    nan_count_test_X = np.isnan(testX_reshaped).sum()\n",
    "    total_train_elements = trainX_reshaped.size\n",
    "    total_test_elements = testX_reshaped.size\n",
    "\n",
    "    if nan_count_train_X > 0 or nan_count_test_X > 0:\n",
    "        nan_percentage_train_X = (nan_count_train_X / total_train_elements) * 100\n",
    "        nan_percentage_test_X = (nan_count_test_X / total_test_elements) * 100\n",
    "        print(f\"Warning: Found NaN values. Train NaNs: {nan_count_train_X} ({nan_percentage_train_X:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_X} ({nan_percentage_test_X:.2f}%)\")\n",
    "\n",
    "        most_frequent_train_X = np.nanmedian(trainX_reshaped)\n",
    "\n",
    "        # Calculate the most frequent value across both samples\n",
    "        combined_data_X = np.concatenate((trainX_reshaped[~np.isnan(trainX_reshaped)],\n",
    "                                          testX_reshaped[~np.isnan(testX_reshaped)]))\n",
    "        most_frequent_value_X = np.bincount(combined_data_X.astype(int)).argmax()\n",
    "\n",
    "        print(\n",
    "            f\"Replacing NaN values with the most frequent value in train: {most_frequent_train_X}, while most frequent overall is  {most_frequent_value_X}\")\n",
    "\n",
    "        trainX_reshaped = np.where(np.isnan(trainX_reshaped), most_frequent_train_X, trainX_reshaped)\n",
    "        testX_reshaped = np.where(np.isnan(testX_reshaped), most_frequent_train_X, testX_reshaped)\n",
    "\n",
    "    train_predictions = model.predict(trainX.reshape(-1, n_timesteps, n_features))\n",
    "    test_predictions = model.predict(testX.reshape(-1, n_timesteps, n_features))\n",
    "\n",
    "    print(\"# of non-Nan predictions for train\", np.count_nonzero(~np.isnan(train_predictions)),\n",
    "          \"of\", train_predictions.size)\n",
    "    print(\"# of non-Nan predictions for test\", np.count_nonzero(~np.isnan(test_predictions)),\n",
    "          \"of\", test_predictions.size)\n",
    "\n",
    "    nan_threshold_y = 1.0  # percent\n",
    "    nan_count_train_y = np.isnan(train_predictions).sum()\n",
    "    nan_count_test_y = np.isnan(test_predictions).sum()\n",
    "\n",
    "    nan_percentage_train_y = (nan_count_train_y / train_predictions.size) * 100\n",
    "    nan_percentage_test_y = (nan_count_test_y / test_predictions.size) * 100\n",
    "\n",
    "    if nan_percentage_train_y > nan_threshold_y or nan_percentage_test_y > nan_threshold_y:\n",
    "        print(f\"Error: NaN percentage in predictions exceeded threshold. \"\n",
    "              f\"Train NaNs: {nan_count_train_y} ({nan_percentage_train_y:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_y} ({nan_percentage_test_y:.2f}%).\")\n",
    "        print(\"Stopping LIME calculations and returning empty results.\")\n",
    "\n",
    "        return np.empty((len(trainX), n_timesteps, n_features)), np.empty(\n",
    "            (len(testX), n_timesteps, n_features))  # return np.array([]), np.array([])\n",
    "    elif nan_percentage_train_y > 0.0 or nan_percentage_test_y > 0.0:\n",
    "        print(f\"Warning: There are NaN's in predictions. \"\n",
    "              f\"Train NaNs: {nan_count_train_y} ({nan_percentage_train_y:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_y} ({nan_percentage_test_y:.2f}%).\")\n",
    "\n",
    "    most_frequent_value_y = np.nanmedian(\n",
    "        train_predictions)  # np.bincount(train_predictions[~np.isnan(train_predictions)].astype(int)).argmax()\n",
    "\n",
    "    def safe_predict_fn(x):\n",
    "        prediction = model.predict(x.reshape(-1, n_timesteps, n_features))\n",
    "\n",
    "        # Check for NaNs in the prediction\n",
    "        if np.isnan(prediction).sum() > 0:\n",
    "            nan_count_prediction = np.isnan(prediction).sum()\n",
    "            nan_percentage_prediction = (nan_count_prediction / prediction.size) * 100\n",
    "            # Replace NaNs in the prediction with a default value  \n",
    "            prediction = np.nan_to_num(prediction, nan=most_frequent_value_y)\n",
    "            print(f\"Warning: Prediction contains NaN values \"\n",
    "                  f\"({nan_count_prediction} NaNs, {nan_percentage_prediction:.2f}%). \"\n",
    "                  f\"Replacing NaN values in prediction with {most_frequent_value_y}.\")\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    feature_names = [f'feature_{i}' for i in range(n_timesteps * n_features)]\n",
    "    class_names = [f'class_{i}' for i in range(model.output_shape[-1])]\n",
    "\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=trainX_reshaped,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode=\"classification\",\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    sv_ts = []\n",
    "    sv_tr = []\n",
    "\n",
    "    for i in range(len(testX)):\n",
    "        # for i in tqdm(range(len(testX)), desc=\"Processing test data\"):\n",
    "        # print(f\"[{i}] testX: start\")\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=testX_reshaped[i],\n",
    "            predict_fn=safe_predict_fn,\n",
    "            num_features=n_timesteps * n_features\n",
    "        )\n",
    "        weights = np.array([feature[1] for feature in explanation.as_list()])\n",
    "        shaped_weights = weights.reshape((n_timesteps, n_features))\n",
    "        sv_ts.append(shaped_weights)\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "        # for i in tqdm(range(len(trainX)), desc=\"Processing train data\"):\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=trainX_reshaped[i],\n",
    "            predict_fn=safe_predict_fn,  # lambda x: model.predict(x.reshape(-1, n_timesteps, n_features))\n",
    "            num_features=n_timesteps * n_features\n",
    "        )\n",
    "        weights = np.array([feature[1] for feature in explanation.as_list()])\n",
    "        shaped_weights = weights.reshape((n_timesteps, n_features))\n",
    "        sv_tr.append(shaped_weights)\n",
    "\n",
    "    return np.array(sv_tr), np.array(sv_ts)\n",
    "\n",
    "\n",
    "def process_dataset(\n",
    "        port_queue: Optional[Queue], port_lock: Optional[Lock], model_name: str, directory: str, model_path: str,\n",
    "        model: Any, available_ports: Optional[List[int]], trainX: np.ndarray, trainy: np.ndarray, testX: np.ndarray,\n",
    "        testy: np.ndarray, bg_size: int, window_len: int, stride: int, explanation_method: str, absshap: bool,\n",
    "        shap_enabled: bool, lime_enabled: bool, anchor_enabled: bool, timeout_duration: int, alt_timeout_duration: int,\n",
    "        parallel: bool = True) -> \\\n",
    "        Tuple[\n",
    "            Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[np.ndarray], Optional[\n",
    "                List[Dict]],\n",
    "            Optional[List[Dict]]]:\n",
    "    print(f\"Processing... {p} with trainX {trainX.shape} and testX {testX.shape} \")\n",
    "\n",
    "    bg_data = load_background_data(trainX, bg_size)  # beginning of dataset\n",
    "\n",
    "    sv_tr, sv_ts = None, None\n",
    "    lime_tr, lime_ts = None, None\n",
    "    anchor_tr, anchor_ts = None, None\n",
    "\n",
    "    if shap_enabled:\n",
    "        if explanation_method == 'window':\n",
    "            print(\"SHAP: window\")\n",
    "            sv_tr, sv_ts = compute_window_shap(model, trainX, testX, window_len, stride, bg_data, trainy.shape[1])\n",
    "        else:\n",
    "            print(\"SHAP: deep\")\n",
    "            sv_tr, sv_ts = compute_deep_shap(model, trainX, testX, bg_data, absshap)\n",
    "\n",
    "    if lime_enabled:\n",
    "        print(\"LIME: deep\")\n",
    "        lime_tr, lime_ts = compute_deep_lime(model, trainX, testX)\n",
    "\n",
    "    if anchor_enabled:\n",
    "        print(\"ANCHOR: ...\")\n",
    "        anchor_tr, anchor_ts = compute_anchor(port_queue, port_lock, model_name, directory, model_path, model,\n",
    "                                              available_ports, trainX, testX, bg_data, parallel, timeout_duration,\n",
    "                                              alt_timeout_duration, max_attempts=5)\n",
    "\n",
    "    return sv_tr, sv_ts, lime_tr, lime_ts, anchor_tr, anchor_ts"
   ],
   "id": "4ce8f52b5e6198dd",
   "outputs": [],
   "execution_count": 15
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## ANCHOR",
   "id": "947e1927033bd8fa"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "### Parallel with TensorFlow Serving",
   "id": "c403147c62b909b0"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T10:10:34.896027Z",
     "start_time": "2024-11-14T10:10:34.845687Z"
    }
   },
   "cell_type": "code",
   "source": [
    "#    source ~/miniconda3/bin/activate\n",
    "#    conda create -n tensorflow-serving-env python=3.9 -y\n",
    "#    conda activate tensorflow-serving-env\n",
    "#    conda install tensorflow tensorflow-gpu psutil flask requests -y\n",
    "#    python -c \"import tensorflow as tf; print(tf.config.list_physical_devices('GPU'))\"\n"
   ],
   "id": "4871d0ffc7d14895",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### ModelServingClient",
   "id": "1184b2f5d0f24558"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:49.353524Z",
     "start_time": "2025-03-18T11:07:49.267541Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class ModelServingClient:\n",
    "    def __init__(self, base_url: str, base_port: int, predict_url: str = \"http://172.16.216.132\", timeout: int = 30):\n",
    "        self.base_url = base_url\n",
    "        self.base_port = base_port\n",
    "        self.predict_url = predict_url\n",
    "        self.timeout = timeout\n",
    "        self.used_ports: Set = set()  # Track used ports\n",
    "        self.checked_ports: Set = set()\n",
    "        self.used_ports_info = []\n",
    "        self.method: Optional[str] = None\n",
    "        self.grow_gpu_mem: Optional[str] = None\n",
    "        self.model: Optional[str] = None\n",
    "\n",
    "    def update_used_ports(self, model_name: Optional[str] = None):\n",
    "        \"\"\"Fetch active serving containers and update self.used_ports.\"\"\"\n",
    "        url = f\"{self.base_url}/active_serving_containers\"\n",
    "        try:\n",
    "            response = requests.get(url, timeout=self.timeout)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            containers = response.json()\n",
    "\n",
    "            # Update self.used_ports with model details\n",
    "            self.used_ports.clear()\n",
    "            self.used_ports_info = []\n",
    "            for container in containers:\n",
    "                if model_name is None or model_name == container['model_name']:\n",
    "                    self.used_ports.add(container['port'])\n",
    "\n",
    "                    self.used_ports_info.append({\n",
    "                        \"model_name\": container[\"model_name\"],\n",
    "                        \"gpu\": container[\"gpu\"],\n",
    "                        \"port\": container[\"port\"],\n",
    "                        \"container_id\": container[\"container_id\"],\n",
    "                        \"pid\": container[\"pid\"]},\n",
    "                    )\n",
    "                    if self.model is not None and self.model != container[\"model_name\"]:\n",
    "                        print(f\"Model {self.model} mismatch with container ({container['model_name']})!\")\n",
    "\n",
    "            print(\"Updated used ports:\", self.used_ports_info)\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to fetch active serving containers with model {model_name}: {e}\")\n",
    "\n",
    "    def find_available_port(self, actual_port: Optional[int] = None) -> int:\n",
    "        \"\"\"Finds the first available port starting from base_port.\"\"\"\n",
    "        port = actual_port or self.base_port\n",
    "        while True:\n",
    "            if port not in self.checked_ports and port not in self.used_ports:\n",
    "                response = requests.get(f\"{self.base_url}/check_ports\", params={\"port\": port}, timeout=self.timeout)\n",
    "                if response.status_code == 200:\n",
    "                    self.used_ports.add(port)\n",
    "                    return port\n",
    "                self.checked_ports.add(port)\n",
    "            port += 2\n",
    "\n",
    "    def get_gpu_with_most_memory(self) -> int:\n",
    "        \"\"\"Returns the GPU index with the most available memory.\"\"\"\n",
    "        response = requests.get(f\"{self.base_url}/gpu_memory\", timeout=self.timeout)\n",
    "        memory_info = response.json()\n",
    "        print(\"memory_info:\", memory_info)\n",
    "        return max(memory_info, key=lambda gpu: int(memory_info[gpu].replace(\" MiB\", \"\")))\n",
    "\n",
    "    def has_sufficient_gpu_memory(self, gpu: str, required_memory_gb: int) -> bool:\n",
    "        \"\"\"\n",
    "        Checks if the specified GPU has at least the required amount of free memory.\n",
    "    \n",
    "        Parameters:\n",
    "        - gpu (str): The GPU ID to check memory for.\n",
    "        - required_memory_gb (int): Minimum required memory in GB.\n",
    "    \n",
    "        Returns:\n",
    "        - bool: True if the GPU has sufficient memory, False otherwise.\n",
    "        \"\"\"\n",
    "        try:\n",
    "            response = requests.get(f\"{self.base_url}/gpu_memory\", timeout=self.timeout)\n",
    "            response.raise_for_status()\n",
    "            memory_info = response.json()\n",
    "\n",
    "            # Sprawdź, czy wskazany GPU jest w odpowiedzi\n",
    "            if gpu not in memory_info:\n",
    "                print(f\"GPU {gpu} not found in memory info: {memory_info}\")\n",
    "                return False\n",
    "\n",
    "            free_memory_mib = int(memory_info[gpu].replace(\" MiB\", \"\"))\n",
    "\n",
    "            free_memory_gb = free_memory_mib / 1024\n",
    "            return free_memory_gb >= required_memory_gb\n",
    "\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Error fetching GPU memory info: {str(e)}\")\n",
    "            return False\n",
    "        except ValueError as e:\n",
    "            print(f\"Error parsing GPU memory info: {str(e)}\")\n",
    "            return False\n",
    "\n",
    "    def load_models(self, model_directory: str, model_name: str, count: int, data: np.ndarray, method: str = \"docker\",\n",
    "                    grow_gpu_mem: str = \"true\", gpu: Optional[str] = None, num_samples: int = 1_000,\n",
    "                    req_free_memory_gb=1) -> Tuple[\n",
    "        List[Dict[str, Any]], List[int]]:\n",
    "        \"\"\"Loads specified number of models with selected resources and returns details.\"\"\"\n",
    "        self.method = method\n",
    "        self.grow_gpu_mem = grow_gpu_mem\n",
    "        self.model = None\n",
    "\n",
    "        loaded_models_info = []\n",
    "        model_path = os.path.join(\"/opt/jupyterhub/fast/shared_dir/UCI-Benchmark\", 'ds', model_directory, model_name,\n",
    "                                  \"model_tf\")\n",
    "\n",
    "        for _ in range(count):\n",
    "            port = self.find_available_port()\n",
    "            final_gpu = gpu or str(self.get_gpu_with_most_memory()) or \"all\"\n",
    "\n",
    "            if final_gpu != \"all\" and not self.has_sufficient_gpu_memory(final_gpu,\n",
    "                                                                         required_memory_gb=req_free_memory_gb):\n",
    "                print(f\"Insufficient GPU memory on {final_gpu}. Skipping model allocation.\")\n",
    "                continue\n",
    "\n",
    "            model_info = self.load_model(model_name, model_path, port, final_gpu, method, grow_gpu_mem)\n",
    "            if model_info[\"status\"] == \"loaded\":\n",
    "                time.sleep(3)\n",
    "                self.wait_for_successful_prediction(model_info[\"port\"], final_gpu, method, model_name, data,\n",
    "                                                    num_samples)\n",
    "                self.used_ports.add(port)\n",
    "                self.model = model_name\n",
    "\n",
    "            loaded_models_info.append(model_info)\n",
    "            time.sleep(1)\n",
    "\n",
    "        return loaded_models_info, list(self.used_ports)\n",
    "\n",
    "    def load_model(self, model_name: str, model_path: str, port: int, gpu: str, method: str, grow_gpu_mem: str) -> Dict[\n",
    "        str, Any]:\n",
    "        \"\"\"Helper function to send model loading request and return load status.\"\"\"\n",
    "        if self.model is None:\n",
    "            self.model = model_name\n",
    "        else:\n",
    "            if self.model != model_name:\n",
    "                raise Exception(f\"Requested model {model_name} does not match defined self.model= {self.model}.\")\n",
    "\n",
    "        start_time = time.time()\n",
    "        load_url = f\"{self.base_url}/load_model\"\n",
    "        data = {\n",
    "            \"model_name\": model_name,\n",
    "            \"model_path\": model_path,\n",
    "            \"port\": port,\n",
    "            \"gpu\": gpu,\n",
    "            \"method\": method,\n",
    "            \"grow_gpu_mem\": grow_gpu_mem,\n",
    "        }\n",
    "\n",
    "        response = requests.post(load_url, json=data, timeout=self.timeout)\n",
    "        try:\n",
    "            response_json = response.json()\n",
    "        except ValueError:\n",
    "            response_json = {\"error\": response.text or \"Response is not in JSON format\"}\n",
    "\n",
    "        if response.status_code == 200:\n",
    "            return {\n",
    "                \"model_name\": model_name,\n",
    "                \"port\": port,\n",
    "                \"gpu\": gpu,\n",
    "                \"load_time\": time.time() - start_time,\n",
    "                \"status\": \"loaded\"\n",
    "            }\n",
    "        else:\n",
    "            return {\n",
    "                \"model_name\": model_name,\n",
    "                \"port\": port,\n",
    "                \"status\": \"failed\",\n",
    "                \"error\": response_json.get(\"error\", \"Unknown error\")\n",
    "            }\n",
    "\n",
    "    def wait_for_successful_prediction(self, port: int, gpu: str, method: str, model_name: str, data: np.ndarray,\n",
    "                                       num_samples) -> None:\n",
    "        \"\"\"Wait until a successful prediction is returned by the model.\"\"\"\n",
    "        check_url = f\"{self.predict_url}:{port}/v1/models/{model_name}\"\n",
    "        predict_url = f\"{self.predict_url}:{port}/v1/models/{model_name}:predict\"\n",
    "        success = False\n",
    "        # Powiel dane, aby zwiększyć zużycie pamięci\n",
    "        random_samples = np.random.normal(0, 1, (num_samples, *data.shape[1:]))\n",
    "        test_data = np.concatenate([data, random_samples], axis=0)\n",
    "        print(\"method:\", method, \", port:\", port, \", gpu:\", gpu, \", model_name:\", model_name, \", test_data:\",\n",
    "              test_data.shape)\n",
    "        while not success:\n",
    "            try:\n",
    "                response = requests.get(check_url, timeout=self.timeout)\n",
    "                if response.status_code == 200:\n",
    "                    model_info = response.json()\n",
    "                    if model_info.get(\"model_version_status\", [{}])[0].get(\"state\") == \"AVAILABLE\":\n",
    "                        response = requests.post(predict_url, data=json.dumps({\"instances\": test_data.tolist()}),\n",
    "                                                 timeout=self.timeout)\n",
    "                        if response.status_code == 200:\n",
    "                            success = True\n",
    "                        if response.status_code == 400 and \"Input to reshape is a tensor with\" in response.json().get(\n",
    "                                \"error\", \"\"):\n",
    "                            raise Exception(response.json().get(\"error\", \"\"))\n",
    "                if not success:\n",
    "                    print(response, response.json())\n",
    "                    time.sleep(1)\n",
    "            except (requests.RequestException, requests.ConnectionError) as e:\n",
    "                print(f\"Error while waiting for prediction: {e}\")\n",
    "                time.sleep(5)\n",
    "\n",
    "    def reload_models(self, model_directory: str, model_name: str, method: str = \"docker\",\n",
    "                      ports: Optional[List[int]] = None) -> List[int]:\n",
    "        \"\"\"Reloads models on all used ports and returns list of successful ports.\"\"\"\n",
    "        reloaded_ports = set()\n",
    "        self.model = None\n",
    "        model_path = os.path.join(\"/opt/jupyterhub/fast/shared_dir/UCI-Benchmark\", 'ds', model_directory, model_name,\n",
    "                                  \"model_tf\")\n",
    "\n",
    "        if ports is None:\n",
    "            ports = list(self.used_ports)\n",
    "\n",
    "        for port in ports:\n",
    "            reload_url = f\"{self.base_url}/load_model\"\n",
    "            data = {\n",
    "                \"model_name\": model_name,\n",
    "                \"model_path\": model_path,\n",
    "                \"port\": port,\n",
    "                \"method\": method\n",
    "            }\n",
    "            try:\n",
    "                response = requests.post(reload_url, json=data, timeout=self.timeout)\n",
    "\n",
    "                if response.status_code == 200:\n",
    "                    self.model = model_name\n",
    "                    self.used_ports.add(port)\n",
    "                    reloaded_ports.add(port)\n",
    "                else:\n",
    "                    reloaded_ports.remove(port)\n",
    "\n",
    "            except requests.Timeout:\n",
    "                print(f\"Timeout: Port {port} may not be responding.\")\n",
    "\n",
    "        return list(reloaded_ports)\n",
    "\n",
    "    def unload_models(self, ports: Optional[List[int]] = None) -> List[int]:\n",
    "        \"\"\"Unloads models on all used ports and returns list of successful ports.\"\"\"\n",
    "        if ports is None:\n",
    "            ports = list(self.used_ports)\n",
    "\n",
    "        unloaded_ports = set()\n",
    "        for port in ports:\n",
    "            unload_url = f\"{self.base_url}/unload_model\"\n",
    "            data = {\"port\": port}\n",
    "            response = requests.post(unload_url, json=data, timeout=self.timeout)\n",
    "            if response.status_code == 200:\n",
    "                unloaded_ports.add(port)\n",
    "                self.model = None\n",
    "            self.used_ports.discard(port)\n",
    "\n",
    "        return list(unloaded_ports)\n",
    "\n",
    "    def terminate_all_containers(self, model_name: Optional[str] = None) -> dict:\n",
    "        \"\"\"Send a request to terminate all running TensorFlow Serving containers.\"\"\"\n",
    "        url = f\"{self.base_url}/terminate_all_containers\"\n",
    "        payload = {\"model_name\": model_name} if model_name else {}\n",
    "        self.model = None\n",
    "        try:\n",
    "            response = requests.post(url, json=payload, timeout=self.timeout)\n",
    "            response.raise_for_status()  # Raise an error for bad status codes\n",
    "            return response.json()  # Return JSON response with terminated containers\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to terminate all containers: {e}\")\n",
    "            return {\"error\": str(e)}\n",
    "\n",
    "    def predict(self, model_name: str, data: np.ndarray, index: int) -> Tuple[int, Dict[str, Any]]:\n",
    "        \"\"\"Sends a prediction request to the model server using the port determined by index modulo rotation.\"\"\"\n",
    "        if len(self.used_ports):\n",
    "            # port = random.choice(list(self.used_ports)) # Choose a random port from available ports\n",
    "            port = list(self.used_ports)[index % len(self.used_ports)]  # Choose constant\n",
    "            data_dict = {\"instances\": data.tolist()}\n",
    "            prediction_url = f\"{self.predict_url}:{port}/v1/models/{model_name}:predict\"\n",
    "            try:\n",
    "                response = requests.post(prediction_url, data=json.dumps(data_dict), timeout=self.timeout)\n",
    "                if response.status_code == 200:\n",
    "                    return port, response.json()\n",
    "                else:\n",
    "                    return port, {\n",
    "                        \"error\": f\"Failed to get prediction on port {port}. Status code: {response.status_code}\"}\n",
    "\n",
    "            except requests.Timeout:\n",
    "                return port, {\"error\": f\"Prediction request timed out on port {port}\"}\n",
    "\n",
    "            except requests.ConnectionError as e:\n",
    "                # Check for specific \"Connection reset by peer\" error and terminate the container\n",
    "                # if isinstance(e.args[0], requests.exceptions.ConnectionError):\n",
    "                # print(f\"Connection reset by peer on port {port}. Terminating container.\")\n",
    "                # self.terminate_container(port)  # Terminate the container on this port\n",
    "                return port, {\"error\": f\"Connection error on port {port}: {str(e)}\"}\n",
    "\n",
    "            except Exception as e:\n",
    "                return port, {\"error\": f\"Exception on port {port}: {str(e)}\"}\n",
    "\n",
    "        return -1, {\"error\": f\"No port available\"}\n",
    "\n",
    "    def terminate_container(self, port: int) -> None:\n",
    "        \"\"\"Terminate a single container identified by the port.\"\"\"\n",
    "        url = f\"{self.base_url}/terminate_container\"\n",
    "        try:\n",
    "            response = requests.post(url, json={\"port\": port}, timeout=self.timeout)\n",
    "            if response.status_code == 200:\n",
    "                print(f\"Container on port {port} terminated successfully.\")\n",
    "            else:\n",
    "                print(f\"Failed to terminate container on port {port}. Status code: {response.status_code}\")\n",
    "        except requests.RequestException as e:\n",
    "            print(f\"Failed to terminate container on port {port}: {e}\")\n",
    "        self.used_ports.remove(port)  # Remove port from used ports list\n"
   ],
   "id": "6d1b80b4b94e730f",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T10:10:39.084263Z",
     "start_time": "2024-11-14T10:10:39.034108Z"
    }
   },
   "cell_type": "code",
   "source": "client = ModelServingClient(MODEL_MANAGER, base_port=8015)",
   "id": "d7f83ea096346722",
   "outputs": [],
   "execution_count": 14
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T10:10:39.306423Z",
     "start_time": "2024-11-14T10:10:39.213255Z"
    }
   },
   "cell_type": "code",
   "source": "client.get_gpu_with_most_memory()",
   "id": "6ca9b06e9dbc64c1",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_info: {'0': '11209 MiB', '1': '11208 MiB'}\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "'0'"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 15
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:02:26.869864Z",
     "start_time": "2024-11-11T13:02:25.227979Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.terminate_all_containers()\n",
    "client.update_used_ports()"
   ],
   "id": "13b390c332c13a45",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forcefully terminated 6 containers',\n",
       " 'terminated_containers': [{'container_id': '9cf068d1e536',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Wafer',\n",
       "   'pid': 1848066,\n",
       "   'port': 8015},\n",
       "  {'container_id': '9cf068d1e536',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Wafer',\n",
       "   'pid': 1848066,\n",
       "   'port': 8015},\n",
       "  {'container_id': '20f9360fc68a',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Wafer',\n",
       "   'pid': 1844552,\n",
       "   'port': 8019},\n",
       "  {'container_id': '20f9360fc68a',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Wafer',\n",
       "   'pid': 1844552,\n",
       "   'port': 8019},\n",
       "  {'container_id': '731b3625375d',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 1814693,\n",
       "   'port': 8017},\n",
       "  {'container_id': '731b3625375d',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 1814693,\n",
       "   'port': 8017}]}"
      ]
     },
     "execution_count": 290,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 290
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:04:01.021663Z",
     "start_time": "2024-11-11T13:03:07.761724Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.update_used_ports()\n",
    "model_info, tf_model_server_ports = client.load_models(\n",
    "    directory, timeseries, data=instance_new, count=2, grow_gpu_mem=\"true\", method=\"docker\",\n",
    "    gpu=None, num_samples=2_000)\n",
    "print(\"Loaded models:\", model_info)"
   ],
   "id": "cbda22bd2982c326",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated used ports: []\n",
      "method: docker , port: 8015 , gpu: 1 , model_name: SelfRegulationSCP1 , test_data: (2001, 896, 6)\n",
      "Error while waiting for prediction: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "method: docker , port: 8017 , gpu: 1 , model_name: SelfRegulationSCP1 , test_data: (2001, 896, 6)\n",
      "Error while waiting for prediction: ('Connection aborted.', ConnectionResetError(104, 'Connection reset by peer'))\n",
      "Loaded models: [{'model_name': 'SelfRegulationSCP1', 'port': 8015, 'gpu': '1', 'load_time': 7.633980989456177, 'status': 'loaded'}, {'model_name': 'SelfRegulationSCP1', 'port': 8017, 'gpu': '1', 'load_time': 5.750410318374634, 'status': 'loaded'}]\n"
     ]
    }
   ],
   "execution_count": 292
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:04:18.230805Z",
     "start_time": "2024-11-11T13:04:18.173239Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# tf_model_server_ports = [mi['port'] for mi in model_info]\n",
    "tf_model_server_ports"
   ],
   "id": "a11b9c989f632544",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[8017, 8015]"
      ]
     },
     "execution_count": 294,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 294
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:56:16.535136Z",
     "start_time": "2024-11-11T12:55:54.477410Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf_model_server_ports = client.reload_models(\"univariate\", \"Wafer\")\n",
    "print(\"Ports:\", tf_model_server_ports)"
   ],
   "id": "e8cc0b82befcadd6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Ports: [8019, 8015]\n"
     ]
    }
   ],
   "execution_count": 286
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:56:40.836757Z",
     "start_time": "2024-11-11T12:56:40.698688Z"
    }
   },
   "cell_type": "code",
   "source": [
    "client.update_used_ports()\n",
    "client.used_ports"
   ],
   "id": "1ade7e2009e51b55",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated used ports: [{'model_name': 'Wafer', 'gpu': '0', 'port': 8015, 'container_id': '9cf068d1e536', 'pid': 1848066}, {'model_name': 'Wafer', 'gpu': '0', 'port': 8019, 'container_id': '20f9360fc68a', 'pid': 1844552}, {'model_name': 'SelfRegulationSCP1', 'gpu': '1', 'port': 8017, 'container_id': '731b3625375d', 'pid': 1814693}]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{8015, 8017, 8019}"
      ]
     },
     "execution_count": 287,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 287
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T13:04:33.403940Z",
     "start_time": "2024-11-11T13:04:33.198588Z"
    }
   },
   "cell_type": "code",
   "source": "client.predict(timeseries, instance_new, index_new)",
   "id": "c3b1daa2b7a47e73",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(8017, {'predictions': [[0.991511047, 0.00848888606]]})"
      ]
     },
     "execution_count": 295,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 295
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "unloaded_ports = client.unload_models()\n",
    "print(\"Unloaded ports:\", unloaded_ports)"
   ],
   "id": "d74285b1666ad97e",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-11T12:28:44.277322Z",
     "start_time": "2024-11-11T12:28:42.822812Z"
    }
   },
   "cell_type": "code",
   "source": "client.terminate_all_containers()",
   "id": "bf08cc44fd44fd13",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forcefully terminated 6 containers',\n",
       " 'terminated_containers': [{'container_id': '2d7feccf00af',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1803952,\n",
       "   'port': 8019},\n",
       "  {'container_id': '2d7feccf00af',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1803952,\n",
       "   'port': 8019},\n",
       "  {'container_id': '4b11b5f1897e',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1799945,\n",
       "   'port': 8015},\n",
       "  {'container_id': '4b11b5f1897e',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1799945,\n",
       "   'port': 8015},\n",
       "  {'container_id': 'ec06e54f7771',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1789013,\n",
       "   'port': 8017},\n",
       "  {'container_id': 'ec06e54f7771',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'ArticularyWordRecognition',\n",
       "   'pid': 1789013,\n",
       "   'port': 8017}]}"
      ]
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 272
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-10T22:53:19.946448Z",
     "start_time": "2024-11-10T22:53:19.883198Z"
    }
   },
   "cell_type": "code",
   "source": "client.used_ports",
   "id": "5c2a4869e1e16dfb",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{8501, 8503}"
      ]
     },
     "execution_count": 167,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 167
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "##### Anchor code",
   "id": "6345cb377ce5ca74"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:57.089518Z",
     "start_time": "2025-03-18T11:07:56.985593Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def to_number(value):\n",
    "    if isinstance(value, np.float64):\n",
    "        return float(value)\n",
    "    elif isinstance(value, np.ndarray) and value.size == 1:\n",
    "        return float(value[0])\n",
    "    elif isinstance(value, (int, float)):\n",
    "        return value\n",
    "    else:\n",
    "        raise ValueError(\"Unsupported type: value must be a number or a single-element numpy array\")\n",
    "\n",
    "\n",
    "def convert_alibi_conditions_to_dict(conditions, feature_names):\n",
    "    rule_dict = {}\n",
    "    for feature_name in feature_names:\n",
    "        for rule in conditions:\n",
    "            if f\"{feature_name} \" in f\"{rule} \":\n",
    "                processed_rules = []\n",
    "                for op in [\"> \", \">= \", \"< \", \"<= \"]:\n",
    "                    if op in rule:\n",
    "                        _rules = rule.split(op)\n",
    "                        # print(_rules)\n",
    "                        if len(_rules) == 2:\n",
    "                            if feature_name in _rules[0]:\n",
    "                                processed_rules.append((op + _rules[1]).replace(' ', ''))\n",
    "                            elif feature_name in _rules[1]:\n",
    "                                result = None\n",
    "                                if op == \"> \":\n",
    "                                    result = \"< \"\n",
    "                                elif op == \">= \":\n",
    "                                    result = \"<= \"\n",
    "                                elif op == \"< \":\n",
    "                                    result = \"> \"\n",
    "                                elif op == \"<= \":\n",
    "                                    result = \">= \"\n",
    "                                processed_rules.append((result + _rules[0]).replace(' ', ''))\n",
    "                        elif len(_rules) == 3:\n",
    "                            if feature_name in _rules[1]:\n",
    "                                processed_rules.append((op + _rules[2]).replace(' ', ''))\n",
    "                                result = None\n",
    "                                if op == \"> \":\n",
    "                                    result = \"< \"\n",
    "                                elif op == \">= \":\n",
    "                                    result = \"<= \"\n",
    "                                elif op == \"< \":\n",
    "                                    result = \"> \"\n",
    "                                elif op == \"<= \":\n",
    "                                    result = \">= \"\n",
    "                                processed_rules.append((result + _rules[0]).replace(' ', ''))\n",
    "\n",
    "                rule_dict[feature_name] = processed_rules\n",
    "\n",
    "    return rule_dict\n",
    "\n",
    "\n",
    "def generate_anchor_explanation(\n",
    "        model_name: str, directory: str, index, ts_df, train_data: np.ndarray, serv_port: int,\n",
    "        serv_client: ModelServingClient, model_path: str, class_names: List[str], feature_names, n_timesteps,\n",
    "        n_features,\n",
    "        anchor_threshold: Optional[float], alt_method: bool, max_attempts=3, timeout_dur: int = 25, do_print=False,\n",
    "        not_scaled_instance=None, prediction=None):\n",
    "    \"\"\"\n",
    "    Generates an anchor explanation for a selected observation in a DataFrame.\n",
    "\n",
    "    :param model_name: \n",
    "    :param directory: \n",
    "    :param index: Index of the observation to explain in the DataFrame.\n",
    "    :param ts_df: DataFrame containing the observations.\n",
    "    :param train_data: np.ndarray.\n",
    "    :param serv_port: Port of the model server's prediction endpoint.\n",
    "    :param serv_client: \n",
    "    :param model_path: \n",
    "    :param class_names: \n",
    "    :param feature_names: List of all feature names.\n",
    "    :param n_timesteps: Number of timesteps in the input data.\n",
    "    :param n_features: Number of features in the input data.\n",
    "    :param alt_method: \n",
    "    :param max_attempts: Maximum number of attempts to generate a satisfactory explanation.\n",
    "    :param timeout_dur: \n",
    "    :param do_print: If True, prints the generated explanation and associated metrics.\n",
    "    :param not_scaled_instance: No need to pass ts_df. \n",
    "    :param prediction: Predicted label for the observation; if None, the model's prediction will be used.\n",
    "\n",
    "    :return: Dictionary containing the generated explanation, including the rule, predicted class, and confidence score.\n",
    "    \"\"\"\n",
    "    # print(f\"Generating anchor explanation for {index}\")\n",
    "    explanation = {\"index\": index, \"success\": False}\n",
    "\n",
    "    def server_predict(data: np.ndarray) -> np.ndarray:\n",
    "        \"\"\"\n",
    "        Sends data to a TensorFlow model server and retrieves the prediction.\n",
    "    \n",
    "        :param data: Input data as a NumPy array, compatible with the model's expected shape.\n",
    "        :return: Model predictions as a NumPy array, in the same format as TensorFlow's `model.predict`.\n",
    "        \"\"\"\n",
    "\n",
    "        serv_port = -1\n",
    "        repetition = 0\n",
    "        while True:\n",
    "            try:\n",
    "                serv_port, response = serv_client.predict(model_name, data, index)\n",
    "                if \"predictions\" in response:\n",
    "                    # print(f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Prediction successful. Port - {serv_port}\")\n",
    "                    if repetition > 10:\n",
    "                        print(\n",
    "                            f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {repetition + 1} - Recovery SUCCESS, port - {serv_port}\")\n",
    "                    break\n",
    "                else:\n",
    "                    print(f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Attempt {repetition + 1}: \"\n",
    "                          f\"! Server returned status {response['error']}. Port - {serv_port}\")\n",
    "                    pass\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Attempt {repetition + 1}: \"\n",
    "                      f\"Request exception - {e}. Port - {serv_port}\")\n",
    "                serv_port = -1\n",
    "\n",
    "            if repetition % 20 == 0 and repetition > 0:\n",
    "                print(\n",
    "                    f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {repetition + 1} - Will REBOOT SERVER {serv_port}\")\n",
    "                if serv_port > 0:\n",
    "                    serv_client.terminate_container(serv_port)\n",
    "                    time.sleep(5)\n",
    "                else:\n",
    "                    serv_port = serv_client.base_port\n",
    "\n",
    "                serv_client.update_used_ports(model_name)\n",
    "\n",
    "                serv_client.used_ports.discard(serv_port)\n",
    "                serv_client.checked_ports.discard(serv_port)\n",
    "                port = serv_client.find_available_port(serv_port)\n",
    "\n",
    "                method = serv_client.method or \"docker\"\n",
    "                grow_gpu_mem = serv_client.grow_gpu_mem or \"true\"\n",
    "                gpu = serv_client.get_gpu_with_most_memory()\n",
    "                print(\n",
    "                    f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - {repetition + 1} - Kill server on port {serv_port} and create on {port}, gpu {gpu}, method {method}\")\n",
    "\n",
    "                external_model_path = os.path.join(\"/opt/jupyterhub/fast/shared_dir/UCI-Benchmark\", 'ds', directory,\n",
    "                                                   model_name,\n",
    "                                                   \"model_tf\")\n",
    "\n",
    "                serv_client.load_model(model_name, external_model_path, port, gpu, method, grow_gpu_mem)\n",
    "\n",
    "            repetition += 1\n",
    "            time.sleep(5)\n",
    "\n",
    "        predictions = response.get(\"predictions\", None)\n",
    "        if predictions is None:\n",
    "            info = f\"! STOP {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - No predictions found in the server response. Port - {serv_port}\"\n",
    "            print(info)\n",
    "            sys.exit(info)\n",
    "            # raise ValueError(\"No predictions found in the server response.\")\n",
    "\n",
    "        predictions = np.array(predictions)\n",
    "\n",
    "        if random.random() < .0001:\n",
    "            print(str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')), \"-\", serv_port, \": alive, observ. index:\", index,\n",
    "                  \", prediction shape:\", predictions.shape, \", up to 3 first values:\", predictions.flatten()[:3])\n",
    "        return predictions\n",
    "\n",
    "    predict_fn = lambda x: server_predict(x.reshape(-1, n_timesteps, n_features)).argmax(axis=1)\n",
    "    if not alt_method:\n",
    "        explainer = AnchorTabular(predict_fn, feature_names)\n",
    "        explainer.fit(train_data)  # , disc_perc=(25, 50, 75)\n",
    "    else:\n",
    "        explainer = anchor_tabular.AnchorTabularExplainer(\n",
    "            class_names=class_names,\n",
    "            feature_names=feature_names,\n",
    "            train_data=train_data,\n",
    "            discretizer='quartile')\n",
    "        print(str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')), \"-\", serv_port, \"-\",\n",
    "              \"Using ALTERNATIVE explainer: observ. index:\", index, \", explainer:\", \"marcotcr\")\n",
    "    try:\n",
    "        instance_values = not_scaled_instance if not_scaled_instance is not None else ts_df.loc[\n",
    "            index, feature_names].values\n",
    "\n",
    "        if prediction is None:\n",
    "            prediction = server_predict(instance_values.reshape(1, n_timesteps, n_features)).argmax(axis=1)[0]\n",
    "        prediction = str(int(prediction))\n",
    "        explanation[\"prediction\"] = prediction\n",
    "        if index < 3:\n",
    "            print(f\"Prediction for {index} is {prediction}\")\n",
    "\n",
    "        best_exp = None\n",
    "        best_metric_for_exp = -1\n",
    "\n",
    "        increment_step = (anchor_threshold + 0.5) / max_attempts\n",
    "        for attempt in range(max_attempts):\n",
    "            if anchor_threshold is not None:\n",
    "                current_threshold = (attempt + 1) * increment_step\n",
    "                threshold_value = min(current_threshold, anchor_threshold)\n",
    "                if not alt_method:\n",
    "                    exp = explainer.explain(instance_values, threshold=threshold_value)\n",
    "                else:\n",
    "                    exp = explainer.explain_instance(instance_values, predict_fn, threshold=threshold_value)\n",
    "            else:\n",
    "                if not alt_method:\n",
    "                    exp = explainer.explain(instance_values)\n",
    "                else:\n",
    "                    exp = explainer.explain_instance(instance_values, predict_fn)\n",
    "            if not alt_method:\n",
    "                coverage, precision, exp_count = to_number(exp.coverage), to_number(exp.precision), len(exp.anchor)\n",
    "            else:\n",
    "                coverage, precision, exp_count = exp.coverage(), exp.precision(), len(exp.names())\n",
    "            if coverage * precision * exp_count > best_metric_for_exp:\n",
    "                best_exp = exp\n",
    "                best_metric_for_exp = coverage * precision * exp_count\n",
    "\n",
    "        if do_print:\n",
    "            print(\"For observation #\", index, \"prediction =\", f\"Class {prediction}\")\n",
    "            print('Anchor: %s' % (' AND '.join(best_exp.anchor)))\n",
    "            print('Precision: %.2f' % best_exp.precision)\n",
    "            print('Coverage: %.2f' % best_exp.coverage)\n",
    "\n",
    "        if best_exp is not None:\n",
    "            if not alt_method:\n",
    "                coverage, precision, exp_count, rules = to_number(best_exp.coverage), to_number(\n",
    "                    best_exp.precision), len(best_exp.anchor), best_exp.anchor\n",
    "            else:\n",
    "                coverage, precision, exp_count, rules = best_exp.coverage(), best_exp.precision(), len(\n",
    "                    best_exp.names()), best_exp.names()\n",
    "            rule_dict = convert_alibi_conditions_to_dict(rules, feature_names)\n",
    "            explanation['rule'] = rule_dict\n",
    "            if not alt_method:\n",
    "                explanation['anchor_method'] = \"alibi\"\n",
    "            else:\n",
    "                explanation['anchor_method'] = \"marcotcr\"\n",
    "            explanation['anchor'] = rules\n",
    "            explanation['confidence'] = precision\n",
    "            explanation['coverage'] = coverage\n",
    "            explanation['exp_count'] = exp_count\n",
    "            explanation['success'] = True\n",
    "\n",
    "    except Exception as e:\n",
    "        print(\"!\", str(datetime.now().strftime('%Y-%m-%d %H:%M:%S')), \"-\", serv_port, \": observ. index:\", index,\n",
    "              \", error:\", str(e))\n",
    "        explanation[\"error\"] = str(e)\n",
    "\n",
    "    return [explanation]  # For ANCHOR there is only one rule, but for DICE could be > 1 counterfactuals\n",
    "\n",
    "\n",
    "def anchor_pool_worker(\n",
    "        model_name: str, directory: str, available_ports: Optional[List[int]], msclient: ModelServingClient,\n",
    "        port_queue: Optional[Queue], port_lock: Optional[Lock], index: int, instance_data: np.ndarray,\n",
    "        feature_names: List[str], class_names: List[str], train_data: np.ndarray, n_features: int, n_timesteps: int,\n",
    "        anchor_threshold: Optional[float], max_attempts: int, dataset_info: str, model_path: str, timeout_duration: int,\n",
    "        alt_timeout_duration: int, ):\n",
    "    \"\"\"Wrapper for multiprocessing to generate explanations, with rotated REST calls on available ports.\"\"\"\n",
    "\n",
    "    explanation = [{\"index\": index, \"success\": False}]\n",
    "\n",
    "    word_length = random.randint(3, 5)\n",
    "    worker_name = ''.join(random.choices(string.ascii_uppercase, k=word_length))\n",
    "\n",
    "    for alt_method, t_out in zip([False, False, True, True],\n",
    "                                 [timeout_duration, 2 * timeout_duration, alt_timeout_duration,\n",
    "                                  2 * alt_timeout_duration]):\n",
    "        explanation[0][\"success\"] = False\n",
    "        if not alt_method:\n",
    "            explanation[0]['anchor_method'] = \"alibi\"\n",
    "        else:\n",
    "            print(\n",
    "                f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - starting ALTERNATIVE method for obs. {index} of {dataset_info}.\")\n",
    "            explanation[0]['anchor_method'] = \"marcotcr\"\n",
    "\n",
    "        with ProcessPoolExecutor(max_workers=1) as executor:\n",
    "            future = executor.submit(generate_anchor_explanation,\n",
    "                                     model_name=model_name,\n",
    "                                     directory=directory,\n",
    "                                     index=index,\n",
    "                                     ts_df=None,\n",
    "                                     train_data=train_data,\n",
    "                                     serv_port=-1,\n",
    "                                     serv_client=msclient,\n",
    "                                     model_path=model_path,\n",
    "                                     class_names=class_names,\n",
    "                                     feature_names=feature_names,\n",
    "                                     n_features=n_features,\n",
    "                                     n_timesteps=n_timesteps,\n",
    "                                     anchor_threshold=anchor_threshold,\n",
    "                                     alt_method=alt_method,\n",
    "                                     max_attempts=max_attempts,\n",
    "                                     not_scaled_instance=instance_data)\n",
    "            try:\n",
    "                result = future.result(timeout=t_out)\n",
    "                explanation = result\n",
    "                if explanation[0].get(\"success\", False):\n",
    "                    if index < 3 or random.random() < .01:\n",
    "                        print(\n",
    "                            f\"{datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Explanation for sample {index} of {dataset_info} with alt_method={alt_method}: {explanation}\")\n",
    "                        explanation[0].pop(\"error\", None)\n",
    "                    break\n",
    "            except FuturesTimeoutError:\n",
    "                print(\n",
    "                    f\"! {worker_name} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Timeout for sample {index} of {dataset_info} with alt_method={alt_method}.\")\n",
    "                explanation[0][\"error\"] = \"FuturesTimeoutError\"\n",
    "            except Exception as e:\n",
    "                error_trace = traceback.format_exc()\n",
    "                print(\n",
    "                    f\"! {worker_name} - {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: ERROR in anchor_pool_worker for obs. {index} of {dataset_info} with alt_method={alt_method}: {str(e)}, stack: {error_trace}.\")\n",
    "                explanation[0][\"error\"] = f\"{str(e)},\\n{error_trace}\"\n",
    "\n",
    "    if not explanation[0][\"success\"]:\n",
    "        print(\n",
    "            f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')} - Explanation for sample {index} of {dataset_info}: {explanation}\")\n",
    "\n",
    "    return explanation\n",
    "\n",
    "\n",
    "def compute_anchor(\n",
    "        port_queue: Optional[Queue],\n",
    "        port_lock: Optional[Lock],\n",
    "        model_name: str,\n",
    "        directory: str,\n",
    "        model_path: str,\n",
    "        model: Any,\n",
    "        available_ports: Optional[List[int]],\n",
    "        trainX: np.ndarray,\n",
    "        testX: np.ndarray,\n",
    "        bg_data: np.ndarray,\n",
    "        parallel: bool,\n",
    "        timeout_duration: int,\n",
    "        alt_timeout_duration: int,\n",
    "        max_attempts: int = 3,\n",
    ") -> Tuple[List[Dict[str, Any]], List[Dict[str, Any]]]:\n",
    "    \"\"\"\n",
    "    Compute anchor explanations for train and test sets.\n",
    "\n",
    "    :param port_queue:  \n",
    "    :param port_lock:  \n",
    "    :param model_name:  \n",
    "    :param directory:  \n",
    "    :param model_path:  \n",
    "    :param model: To know the shape.\n",
    "    :param available_ports: List of ports for REST API model servers.\n",
    "    :param trainX: Training dataset (samples x timesteps x features).\n",
    "    :param testX: Test dataset (samples x timesteps x features).\n",
    "    :param bg_data: Background data for the explainer.\n",
    "    :param max_attempts: Maximum number of attempts to generate a satisfactory explanation.\n",
    "\n",
    "    :return: Tuple containing lists of anchor explanations for train and test sets.\n",
    "    \"\"\"\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "    trainX_reshaped = trainX.reshape(-1, n_timesteps * n_features)\n",
    "    testX_reshaped = testX.reshape(-1, n_timesteps * n_features)\n",
    "\n",
    "    # Define feature names and class names\n",
    "    feature_names = [f'feature_{i}' for i in range(n_timesteps * n_features)]\n",
    "    class_names = [f'class_{i}' for i in range(model.output_shape[-1])]\n",
    "    train_data = bg_data.reshape(-1, n_timesteps * n_features)\n",
    "\n",
    "    # Generate anchor explanations for train and test sets\n",
    "    anchor_explanations_train, anchor_explanations_test = [], []\n",
    "    if parallel:\n",
    "        _client = ModelServingClient(MODEL_MANAGER, base_port=8015)\n",
    "        _client.update_used_ports(model_name)\n",
    "        available_ports = list(_client.used_ports)\n",
    "\n",
    "        # Determine the number of cores\n",
    "        # total_cores = multiprocessing.cpu_count()\n",
    "        # num_cores = int(5 / 11 * total_cores)\n",
    "        num_cores = int(11 / 11 * len(available_ports) - 1)  #  5 / 11 * ... - 2\n",
    "        # num_cores = 48\n",
    "        print(f\"Using {num_cores} cores for parallel processing\")\n",
    "\n",
    "        # Parallel computation of anchor explanations\n",
    "        for dataset, dataX_reshaped, anchor_explanations in zip(\n",
    "                [\"Train\", \"Test\"], [trainX_reshaped, testX_reshaped],\n",
    "                [anchor_explanations_train, anchor_explanations_test]):\n",
    "            print(f\"{dataset} set (parallel)\")\n",
    "            with ProcessPoolExecutor(max_workers=num_cores) as executor:\n",
    "                futures = []\n",
    "                for i in range(len(dataX_reshaped)):\n",
    "                    future = executor.submit(anchor_pool_worker,\n",
    "                                             model_name, directory, available_ports, _client, port_queue, port_lock, i,\n",
    "                                             dataX_reshaped[i].reshape(1, -1), feature_names, class_names, train_data,\n",
    "                                             n_features, n_timesteps, 0.95, max_attempts, dataset, model_path,\n",
    "                                             timeout_duration, alt_timeout_duration)\n",
    "                    futures.append((i, future))\n",
    "\n",
    "                final_results = []\n",
    "                for index, future in futures:\n",
    "                    # index = -1\n",
    "                    try:\n",
    "                        explanation = future.result(timeout=(4 * timeout_duration + 4 * alt_timeout_duration + 30))\n",
    "                    except multiprocessing.context.TimeoutError:\n",
    "                        print(\n",
    "                            f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: TIMEOUT in multiprocessing.Pool for obs. {index}.\")\n",
    "                        explanation = [{\"index\": index, \"success\": False, \"error\": \"TIMEOUT in multiprocessing.Pool\"}]\n",
    "\n",
    "                    except Exception as e:\n",
    "                        error_trace = traceback.format_exc()\n",
    "                        print(\n",
    "                            f\"! {datetime.now().strftime('%Y-%m-%d %H:%M:%S')}: ERROR in multiprocessing.Pool for obs. {index}: {str(e)}, stack: {error_trace}.\")\n",
    "                        explanation = [{\"index\": index, \"success\": False, \"error\": f\"{str(e)},\\n{error_trace}\"}]\n",
    "\n",
    "                    if index % 25 == 0 and index > 0:\n",
    "                        print(f\"Saving {dataset}, obs= {index}, explanation= {explanation}\")\n",
    "                        cache = [exp for i, exp in final_results if exp[0][\"success\"]]\n",
    "                        if dataset == \"Train\":\n",
    "                            save_bundle(None, None, None, None, None, cache, None, model_name,\n",
    "                                        dir=os.path.join(data_path, 'ds', directory), explain_prefix=\"CACHE_av\",\n",
    "                                        only_explain=True)\n",
    "                        else:\n",
    "                            save_bundle(None, None, None, None, None, None, cache, model_name,\n",
    "                                        dir=os.path.join(data_path, 'ds', directory), explain_prefix=\"CACHE_av\",\n",
    "                                        only_explain=True)\n",
    "\n",
    "                    final_results.append((index, explanation))\n",
    "\n",
    "            # Sort results by index to ensure the original order is maintained\n",
    "            print(f\"Sorting {dataset}, #\", len(final_results))\n",
    "            final_results.sort(key=lambda x: x[0])\n",
    "            # Collect results \n",
    "            for i, explanation in final_results:\n",
    "                anchor_explanations.append(explanation)\n",
    "            print(\n",
    "                f\"--------------------------------------- {directory} - {model_name} - {dataset}: --------------------------------------- \")\n",
    "            summarize_anchor_explanations(anchor_explanations)\n",
    "\n",
    "        print(\"DONE\")\n",
    "\n",
    "    else:\n",
    "        raise NotImplementedError(\"Serial\")\n",
    "\n",
    "    return anchor_explanations_train, anchor_explanations_test\n",
    "\n",
    "\n",
    "def summarize_anchor_explanations(anchor_explanations: List[Dict[str, any]]) -> None:\n",
    "    \"\"\"\n",
    "    Summarizes success and failure rates of anchor explanations and displays \n",
    "    a breakdown of failure types with percentages.\n",
    "\n",
    "    :param anchor_explanations: List of anchor explanations with each explanation \n",
    "                                containing a \"success\" key and optionally an \"error\" message.\n",
    "    \"\"\"\n",
    "    total_explanations = len(anchor_explanations)\n",
    "    if total_explanations == 0:\n",
    "        print(\"No anchor explanations to summarize.\")\n",
    "        return\n",
    "\n",
    "    # Count successes and failures\n",
    "    success_count = sum(1 for explanations in anchor_explanations if explanations[0].get(\"success\"))\n",
    "    failure_count = total_explanations - success_count\n",
    "\n",
    "    # Group errors by type\n",
    "    error_types = {}\n",
    "    for explanations in anchor_explanations:\n",
    "        explanation = explanations[0]\n",
    "        if not explanation.get(\"success\"):\n",
    "            error_message = explanation.get(\"error\", \"Unknown error\")\n",
    "            if error_message not in error_types:\n",
    "                error_types[error_message] = 0\n",
    "            error_types[error_message] += 1\n",
    "\n",
    "    # Calculate percentages\n",
    "    success_percentage = (success_count / total_explanations) * 100\n",
    "    failure_percentage = (failure_count / total_explanations) * 100\n",
    "\n",
    "    # Display results\n",
    "    print(f\"\\nSummary of Anchor Explanations (Total: {total_explanations}):\")\n",
    "    print(f\"  - Success: {success_count} ({success_percentage:.2f}%)\")\n",
    "    print(f\"  - Failure: {failure_count} ({failure_percentage:.2f}%)\")\n",
    "\n",
    "    # Display failure types with percentages\n",
    "    print(\"\\nFailure Types:\")\n",
    "    for error_type, count in error_types.items():\n",
    "        error_percentage = (count / total_explanations) * 100\n",
    "        print(f\"  - {error_type}: {count} ({error_percentage:.2f}%)\")"
   ],
   "id": "fd8f8026db217ccf",
   "outputs": [],
   "execution_count": 17
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Setup and test",
   "id": "7fb23277bd2d9337"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:07:59.381659Z",
     "start_time": "2025-03-18T11:07:59.323651Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bg_size = 1000\n",
    "window_len = 23\n",
    "stride = 10\n",
    "explanation_method = 'NA'  # or 'window' if needed for SHAP window-based explanations\n",
    "absshap = True\n",
    "shap_enabled = False  # SHAP disabled\n",
    "lime_enabled = False  # LIME enabled\n",
    "anchor_enabled = True  # ANCHOR enabled\n",
    "explain_prefix = \"av\"\n",
    "\n",
    "print(\"path:\", data_path, \", batch:\", batch_size, \", background:\", bg_size, \", window:\", window_len, \", stride:\",\n",
    "      stride, \", method:\", explanation_method, \", absshap:\", absshap, \", SHAP:\", shap_enabled, \", LIME:\", lime_enabled,\n",
    "      \", ANCHOR:\", anchor_enabled, \", prefix:\", explain_prefix)\n",
    "\n",
    "summary = []"
   ],
   "id": "36756162cc0f9fd9",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "path: shared/UCI-Benchmark/ , batch: 64 , background: 1000 , window: 23 , stride: 10 , method: NA , absshap: True , SHAP: False , LIME: False , ANCHOR: True , prefix: av\n"
     ]
    }
   ],
   "execution_count": 18
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-03-18T11:08:03.140735Z",
     "start_time": "2025-03-18T11:08:03.065425Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets_uni_minus_ChlorineConcentration_last_TwoPatterns_UWaveGestureLibraryXXX = [a for a in datasets_uni if\n",
    "                                                                                    a not in [\n",
    "                                                                                        \"ChlorineConcentration\",\n",
    "                                                                                        \"TwoPatterns\",\n",
    "                                                                                        \"UWaveGestureLibraryAll\",\n",
    "                                                                                        \"UWaveGestureLibraryX\",\n",
    "                                                                                        \"UWaveGestureLibraryY\",\n",
    "                                                                                        \"UWaveGestureLibraryZ\",\n",
    "                                                                                    ]] + [\n",
    "                                                                                       \"UWaveGestureLibraryX\",\n",
    "                                                                                       \"UWaveGestureLibraryY\",\n",
    "                                                                                       \"UWaveGestureLibraryZ\",\n",
    "                                                                                       \"UWaveGestureLibraryAll\",\n",
    "                                                                                       \"TwoPatterns\",\n",
    "                                                                                   ]\n",
    "\n",
    "print(datasets_uni_minus_ChlorineConcentration_last_TwoPatterns_UWaveGestureLibraryXXX)"
   ],
   "id": "92982883155e1917",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DodgerLoopGame', 'ProximalPhalanxTW', 'DodgerLoopDay', 'ECGFiveDays', 'UMD', 'Plane', 'ECG200', 'GunPointAgeSpan', 'DiatomSizeReduction', 'MedicalImages', 'Meat', 'Trace', 'MiddlePhalanxOutlineAgeGroup', 'Chinatown', 'DistalPhalanxOutlineAgeGroup', 'WormsTwoClass', 'DistalPhalanxOutlineCorrect', 'Strawberry', 'OliveOil', 'SmoothSubspace', 'Fungi', 'ElectricDevices', 'SwedishLeaf', 'CricketX', 'ECG5000', 'PhalangesOutlinesCorrect', 'FaceFour', 'SyntheticControl', 'FordA', 'PowerCons', 'BeetleFly', 'GunPointMaleVersusFemale', 'Yoga', 'Herring', 'Crop', 'RefrigerationDevices', 'Worms', 'OSULeaf', 'ItalyPowerDemand', 'GunPoint', 'CBF', 'Symbols', 'ToeSegmentation2', 'TwoLeadECG', 'SmallKitchenAppliances', 'ShapesAll', 'ScreenType', 'Computers', 'CricketY', 'SonyAIBORobotSurface2', 'Wafer', 'MiddlePhalanxOutlineCorrect', 'Lightning2', 'Wine', 'ProximalPhalanxOutlineCorrect', 'ShapeletSim', 'CricketZ', 'BME', 'Beef', 'DodgerLoopWeekend', 'ProximalPhalanxOutlineAgeGroup', 'LargeKitchenAppliances', 'FordB', 'FreezerRegularTrain', 'Lightning7', 'BirdChicken', 'GunPointOldVersusYoung', 'Coffee', 'FreezerSmallTrain', 'SonyAIBORobotSurface1', 'Adiac', 'MiddlePhalanxTW', 'Earthquakes', 'DistalPhalanxTW', 'FiftyWords', 'WordSynonyms', 'MoteStrain', 'InsectWingbeatSound', 'UWaveGestureLibraryX', 'UWaveGestureLibraryY', 'UWaveGestureLibraryZ', 'UWaveGestureLibraryAll', 'TwoPatterns']\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:44:28.976566Z",
     "start_time": "2024-11-30T20:44:23.014728Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# p = 'Wafer'\n",
    "# directory = 'univariate'\n",
    "p = \"Libras\"  # 'SelfRegulationSCP1'\n",
    "directory = 'multivariate'\n",
    "\n",
    "model_temp, _, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory),\n",
    "                                                                load_model=True)\n",
    "model_path = os.path.join(data_path, 'ds', directory, p)\n",
    "print(trainX.shape, trainX[:10].shape, model_path)"
   ],
   "id": "95cdf4480fdcfc73",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model in tf format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-11-30 20:44:28.198029: W tensorflow/c/c_api.cc:304] Operation '{name:'AssignVariableOp_330' id:8499 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node AssignVariableOp_330}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](bias_33, Identity_330)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(270, 45, 2) (10, 45, 2) shared/UCI-Benchmark/ds/multivariate/Libras\n"
     ]
    }
   ],
   "execution_count": 44
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:44:37.687407Z",
     "start_time": "2024-11-30T20:44:37.532204Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ms_client = ModelServingClient(MODEL_MANAGER, base_port=8015)\n",
    "ms_client.update_used_ports(p)"
   ],
   "id": "61cc0deb6bd24b2",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated used ports: []\n"
     ]
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:44:01.840891Z",
     "start_time": "2024-11-30T20:43:52.267422Z"
    }
   },
   "cell_type": "code",
   "source": "# ms_client.terminate_all_containers(p)",
   "id": "36ac8b382f089f69",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forcefully terminated 18 containers',\n",
       " 'terminated_containers': [{'container_id': 'ca83faa5d4c2',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3252436,\n",
       "   'port': 8055},\n",
       "  {'container_id': 'e4cee15b84ef',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3249115,\n",
       "   'port': 8053},\n",
       "  {'container_id': '5ee470892995',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3245749,\n",
       "   'port': 8051},\n",
       "  {'container_id': 'a2b7b4b043ca',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3242367,\n",
       "   'port': 8049},\n",
       "  {'container_id': 'eb715190ed2a',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3238995,\n",
       "   'port': 8047},\n",
       "  {'container_id': 'd01c53a7ed52',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3235660,\n",
       "   'port': 8045},\n",
       "  {'container_id': 'b2814806b585',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3232285,\n",
       "   'port': 8043},\n",
       "  {'container_id': 'cbaab875a5d5',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3228904,\n",
       "   'port': 8041},\n",
       "  {'container_id': 'ce2005c86369',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3225530,\n",
       "   'port': 8039},\n",
       "  {'container_id': 'ff45793ed18f',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3222181,\n",
       "   'port': 8037},\n",
       "  {'container_id': 'c70a5c7f05f3',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3218802,\n",
       "   'port': 8035},\n",
       "  {'container_id': '29a02af6d1f1',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3215441,\n",
       "   'port': 8033},\n",
       "  {'container_id': '85879ba5924b',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3212082,\n",
       "   'port': 8031},\n",
       "  {'container_id': '1f79a25bd9ad',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3208725,\n",
       "   'port': 8029},\n",
       "  {'container_id': 'deeb9e1af70a',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3205362,\n",
       "   'port': 8027},\n",
       "  {'container_id': 'a2f9c6d5cd2c',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3202003,\n",
       "   'port': 8025},\n",
       "  {'container_id': 'bddaafda0014',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3198583,\n",
       "   'port': 8023},\n",
       "  {'container_id': 'd8196bba64d9',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP1',\n",
       "   'pid': 3195218,\n",
       "   'port': 8021}]}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 43
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T21:11:20.886677Z",
     "start_time": "2024-11-30T21:09:27.982548Z"
    }
   },
   "cell_type": "code",
   "source": [
    "last_instance = trainX[-1]\n",
    "last_instance_for_predict = np.expand_dims(last_instance, axis=0)\n",
    "model_info, tf_model_server_ports = ms_client.load_models(\n",
    "    directory, p, data=last_instance_for_predict, count=9 * 2, grow_gpu_mem=\"true\", method=\"docker\", gpu=None,\n",
    "    num_samples=1_000, req_free_memory_gb=1)"
   ],
   "id": "c8001aa06d009646",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_info: {'0': '15512 MiB', '1': '15638 MiB'}\n",
      "method: docker , port: 8091 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '15512 MiB', '1': '15284 MiB'}\n",
      "method: docker , port: 8093 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '15095 MiB', '1': '15284 MiB'}\n",
      "method: docker , port: 8095 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '15095 MiB', '1': '14931 MiB'}\n",
      "method: docker , port: 8097 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '14678 MiB', '1': '14931 MiB'}\n",
      "method: docker , port: 8099 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '14678 MiB', '1': '14578 MiB'}\n",
      "method: docker , port: 8101 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '14261 MiB', '1': '14578 MiB'}\n",
      "method: docker , port: 8103 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '14261 MiB', '1': '14225 MiB'}\n",
      "method: docker , port: 8105 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13906 MiB', '1': '14225 MiB'}\n",
      "method: docker , port: 8107 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13906 MiB', '1': '13870 MiB'}\n",
      "method: docker , port: 8109 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13488 MiB', '1': '13870 MiB'}\n",
      "method: docker , port: 8111 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13488 MiB', '1': '13452 MiB'}\n",
      "method: docker , port: 8113 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13135 MiB', '1': '13452 MiB'}\n",
      "method: docker , port: 8115 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '13135 MiB', '1': '13099 MiB'}\n",
      "method: docker , port: 8117 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '12782 MiB', '1': '13099 MiB'}\n",
      "method: docker , port: 8119 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '12782 MiB', '1': '12682 MiB'}\n",
      "method: docker , port: 8121 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '12365 MiB', '1': '12682 MiB'}\n",
      "method: docker , port: 8123 , gpu: 1 , model_name: Libras , test_data: (1001, 45, 2)\n",
      "memory_info: {'0': '12365 MiB', '1': '12265 MiB'}\n",
      "method: docker , port: 8125 , gpu: 0 , model_name: Libras , test_data: (1001, 45, 2)\n"
     ]
    }
   ],
   "execution_count": 49
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:46:42.042716Z",
     "start_time": "2024-11-30T20:46:41.968352Z"
    }
   },
   "cell_type": "code",
   "source": [
    "available_ports = list(ms_client.used_ports)\n",
    "if len(available_ports) == 0:\n",
    "    raise Exception(\"No available ports\")\n",
    "print(\"We have \", len(available_ports), \"available ports\")"
   ],
   "id": "2457b25c1339f5c3",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "We have  18 available ports\n"
     ]
    }
   ],
   "execution_count": 47
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T20:27:00.722584Z",
     "start_time": "2024-11-30T20:27:00.334322Z"
    }
   },
   "cell_type": "code",
   "source": "ms_client.terminate_all_containers(\"test\")",
   "id": "17e3c2804aea0a0a",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forcefully terminated 0 containers', 'terminated_containers': []}"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "test_N = 10\n",
    "sv_tr, sv_ts, lime_tr, lime_ts, anchor_tr, anchor_ts = process_dataset(\n",
    "    None, None, p, directory, model_path, model_temp, available_ports, trainX[:test_N], trainy[:test_N], testX[:test_N],\n",
    "    testy[:test_N], bg_size, window_len, stride, explanation_method, absshap, shap_enabled, lime_enabled,\n",
    "    anchor_enabled, timeout_duration=60, alt_timeout_duration=60, parallel=True)\n",
    "\n",
    "print(f'Saving bundle... {p},  SHAP: {shap_enabled}, LIME: {lime_enabled}, ANCHOR: {anchor_enabled}')\n",
    "save_bundle(None, trainX, trainy, testX, testy, anchor_tr, anchor_ts, p,\n",
    "            dir=os.path.join(data_path, 'ds', directory), explain_prefix=\"TEST_\" + explain_prefix, only_explain=True)"
   ],
   "id": "b9905f73e6a99931",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-14T09:28:53.928371Z",
     "start_time": "2024-11-14T09:28:53.850366Z"
    }
   },
   "cell_type": "code",
   "source": "anchor_ts",
   "id": "9afb73e8be24b220",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[[{'index': -2, 'success': False, 'error': 'TIMEOUT in multiprocessing.Pool'}],\n",
       " [{'index': -8, 'success': False, 'error': 'TIMEOUT in multiprocessing.Pool'}],\n",
       " [{'index': 0,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 1,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 3,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 4,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 5,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 6,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 7,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}],\n",
       " [{'index': 9,\n",
       "   'success': True,\n",
       "   'prediction': '1',\n",
       "   'rule': {},\n",
       "   'anchor_method': 'alibi',\n",
       "   'anchor': [],\n",
       "   'confidence': 1.0,\n",
       "   'coverage': 1,\n",
       "   'exp_count': 0}]]"
      ]
     },
     "execution_count": 54,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 54
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:04:33.536855Z",
     "start_time": "2024-11-17T09:04:33.374815Z"
    }
   },
   "cell_type": "code",
   "source": "p not in ['Wafer', 'DodgerLoopGame']",
   "id": "6df4cfb1d10dca02",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "False"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 73
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T18:12:06.175843Z",
     "start_time": "2024-11-30T18:12:03.083143Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ms_client = ModelServingClient(MODEL_MANAGER, base_port=8015)\n",
    "if p not in ['Wafer', 'DodgerLoopGame']:\n",
    "    ms_client.terminate_all_containers(p)\n",
    "ms_client.update_used_ports()\n",
    "print(\"We have \", len(list(ms_client.used_ports)), \"available ports\")"
   ],
   "id": "9df781398742b91d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Updated used ports: []\n",
      "We have  0 available ports\n"
     ]
    }
   ],
   "execution_count": 19
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:06:43.106172Z",
     "start_time": "2024-11-17T09:06:42.753907Z"
    }
   },
   "cell_type": "code",
   "source": [
    "ms_client.get_gpu_with_most_memory()\n",
    "print(ms_client.has_sufficient_gpu_memory(\"0\", 1))\n",
    "print(ms_client.has_sufficient_gpu_memory(\"0\", 2))\n",
    "print(ms_client.has_sufficient_gpu_memory(\"1\", 1))\n"
   ],
   "id": "273d6e1317cf9727",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_info: {'0': '1960 MiB', '1': '631 MiB'}\n",
      "True\n",
      "False\n",
      "False\n"
     ]
    }
   ],
   "execution_count": 83
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-17T09:08:32.153830Z",
     "start_time": "2024-11-17T09:08:17.762447Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ms_client.load_models(directory, p, data=last_instance_for_predict, count=1, grow_gpu_mem=\"true\", method=\"docker\",\n",
    "#                       gpu=None, num_samples=3_000)"
   ],
   "id": "1188a64450413ff6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_info: {'0': '1960 MiB', '1': '631 MiB'}\n",
      "method: docker , port: 8015 , gpu: 0 , model_name: DodgerLoopGame , test_data: (3001, 288, 1)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "([{'model_name': 'DodgerLoopGame',\n",
       "   'port': 8015,\n",
       "   'gpu': '0',\n",
       "   'load_time': 0.9104182720184326,\n",
       "   'status': 'loaded'}],\n",
       " [8119,\n",
       "  8121,\n",
       "  8123,\n",
       "  8125,\n",
       "  8127,\n",
       "  8129,\n",
       "  8131,\n",
       "  8133,\n",
       "  8135,\n",
       "  8137,\n",
       "  8139,\n",
       "  8141,\n",
       "  8143,\n",
       "  8015,\n",
       "  8145,\n",
       "  8147,\n",
       "  8149,\n",
       "  8151,\n",
       "  8153,\n",
       "  8155,\n",
       "  8157,\n",
       "  8159,\n",
       "  8161,\n",
       "  8163,\n",
       "  8165,\n",
       "  8167,\n",
       "  8169,\n",
       "  8171,\n",
       "  8173,\n",
       "  8175,\n",
       "  8177,\n",
       "  8181,\n",
       "  8183,\n",
       "  8185,\n",
       "  8187,\n",
       "  8189])"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 84
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Univariate",
   "id": "3c334e3e0144b8d9"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-14T10:51:24.521683Z",
     "start_time": "2024-12-14T10:51:15.667135Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# ms_client = ModelServingClient(MODEL_MANAGER, base_port=9015)\n",
    "ms_client.terminate_all_containers()"
   ],
   "id": "c012344663a2fd26",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'message': 'Forcefully terminated 17 containers',\n",
       " 'terminated_containers': [{'container_id': '6dbc3f9357ef',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP2',\n",
       "   'pid': 1925257,\n",
       "   'port': 10023},\n",
       "  {'container_id': '373e2c2dcf97',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1921881,\n",
       "   'port': 9037},\n",
       "  {'container_id': '901a5e593005',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP2',\n",
       "   'pid': 1918704,\n",
       "   'port': 10021},\n",
       "  {'container_id': '3bdfa68de982',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1915404,\n",
       "   'port': 9035},\n",
       "  {'container_id': 'ff6150c5fc64',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP2',\n",
       "   'pid': 1911825,\n",
       "   'port': 10019},\n",
       "  {'container_id': '71394f72ea13',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1908645,\n",
       "   'port': 9033},\n",
       "  {'container_id': '2bede1b98bd0',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'SelfRegulationSCP2',\n",
       "   'pid': 1903457,\n",
       "   'port': 10017},\n",
       "  {'container_id': '4c3b3a3815f5',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1901414,\n",
       "   'port': 9031},\n",
       "  {'container_id': '3129890ed5da',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1895204,\n",
       "   'port': 9029},\n",
       "  {'container_id': 'b74437bd3563',\n",
       "   'gpu': '0',\n",
       "   'model_name': 'SelfRegulationSCP2',\n",
       "   'pid': 1894705,\n",
       "   'port': 10015},\n",
       "  {'container_id': '6cf09f16b7e2',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1890896,\n",
       "   'port': 9027},\n",
       "  {'container_id': '9e10cc7df909',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1887352,\n",
       "   'port': 9025},\n",
       "  {'container_id': 'cac2657846db',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1884057,\n",
       "   'port': 9023},\n",
       "  {'container_id': '6c83310a5b2d',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1880174,\n",
       "   'port': 9021},\n",
       "  {'container_id': '9eec3437009e',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1876372,\n",
       "   'port': 9019},\n",
       "  {'container_id': '0236e6a691b3',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1872596,\n",
       "   'port': 9017},\n",
       "  {'container_id': 'f5f40f2fae34',\n",
       "   'gpu': '1',\n",
       "   'model_name': 'Meat',\n",
       "   'pid': 1868561,\n",
       "   'port': 9015}]}"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 45
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2025-03-18T11:16:25.792455Z"
    }
   },
   "cell_type": "code",
   "source": [
    "N = 10 * 2  # Wafer: 26, DodgerLoopGame: 18\n",
    "directory = 'univariate'\n",
    "ms_client = ModelServingClient(MODEL_MANAGER, base_port=9015)\n",
    "# ms_client.terminate_all_containers()\n",
    "\n",
    "for p in ['Wafer'] + datasets_uni_minus_ChlorineConcentration_last_TwoPatterns_UWaveGestureLibraryXXX:\n",
    "    print(f'Loading... {p}')\n",
    "\n",
    "    explain_tr_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}tr.pickle')\n",
    "    explain_ts_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}ts.pickle')\n",
    "    if os.path.exists(explain_tr_path) and os.path.exists(explain_ts_path):\n",
    "        if os.path.getsize(explain_tr_path) > 1024 and os.path.getsize(explain_ts_path) > 1024:\n",
    "            print(\n",
    "                f\"Omitting {p}, explainers {directory} SHAP:{shap_enabled} LIME:{lime_enabled} ANCHOR:{anchor_enabled} already calculated.\")\n",
    "            continue\n",
    "\n",
    "    model_path = os.path.join(data_path, 'ds', directory, p)\n",
    "    if p not in ['Wafer']:\n",
    "        model_temp, _, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory),\n",
    "                                                                        load_model=True)\n",
    "    last_instance = trainX[-1]\n",
    "    last_instance_for_predict = np.expand_dims(last_instance, axis=0)\n",
    "\n",
    "    if p not in ['Wafer', 'DodgerLoopGame']:\n",
    "        model_info, tf_model_server_ports = ms_client.load_models(\n",
    "            directory, p, data=last_instance_for_predict, count=N, grow_gpu_mem=\"true\", method=\"docker\", gpu=None,\n",
    "            num_samples=100, req_free_memory_gb=1)\n",
    "\n",
    "    ms_client.update_used_ports(p)\n",
    "    available_ports = list(ms_client.used_ports)\n",
    "    if len(available_ports) == 0:\n",
    "        raise Exception(\"No available ports\")\n",
    "    print(\"We have \", len(available_ports), \"available ports\")\n",
    "\n",
    "    sv_tr, sv_ts, lime_tr, lime_ts, anchor_tr, anchor_ts = process_dataset(\n",
    "        None, None, p, directory, model_path, model_temp, available_ports, trainX, trainy, testX, testy,\n",
    "        bg_size, window_len, stride, explanation_method, absshap, shap_enabled, lime_enabled, anchor_enabled,\n",
    "        timeout_duration=int(2 * 60.0 * 60), alt_timeout_duration=int(2 * 60.0 * 60), parallel=True)\n",
    "\n",
    "    # _, score = evaluate_model(model_temp, testX, testy, batch_size, None)\n",
    "    # score = score * 100.0\n",
    "    # print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    print(f'Saving bundle... {p},  SHAP: {shap_enabled}, LIME: {lime_enabled}, ANCHOR: {anchor_enabled}')\n",
    "    save_bundle(None, trainX, trainy, testX, testy, anchor_tr, anchor_ts, p,\n",
    "                dir=os.path.join(data_path, 'ds', directory), explain_prefix=explain_prefix, only_explain=True)\n",
    "    # summary.append([directory, p, score, 'OK'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model_temp, trainX, trainy, testX, testy, sv_tr, sv_ts, lime_tr, lime_ts\n",
    "    gc.collect()\n",
    "    ms_client.terminate_all_containers(p)\n",
    "\n",
    "\n",
    "# Explanation for train sample 0: [{'index': 0, 'success': True, 'prediction': '0', 'rule': {'feature_2': ['<=0.95'], 'feature_20': ['<=0.95'], 'feature_26': ['<=-0.50'], 'feature_200': ['<=0.95'], 'feature_264': ['<=-0.12'], 'feature_265': ['<=-0.50']}, 'confidence': 1.0}]\n",
    "# Explanation for train sample 1: [{'index': 1, 'success': True, 'prediction': '1', 'rule': {'feature_2': ['>0.57'], 'feature_20': ['>1.34'], 'feature_26': ['>0.57'], 'feature_201': ['>1.34'], 'feature_261': ['>0.57']}, 'confidence': 0.9803157093952073}]\n",
    "# Explanation for train sample 2: [{'index': 2, 'success': True, 'prediction': '0', 'rule': {'feature_2': ['>0.72'], 'feature_22': ['>0.72'], 'feature_26': ['<=-0.50'], 'feature_224': ['>0.72'], 'feature_264': ['<=-0.12'], 'feature_265': ['<=-0.50']}, 'confidence': 1.0}]    "
   ],
   "id": "c9a13146e23dc945",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... Wafer\n",
      "Omitting Wafer, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... DodgerLoopGame\n",
      "Omitting DodgerLoopGame, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... ProximalPhalanxTW\n",
      "Omitting ProximalPhalanxTW, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... DodgerLoopDay\n",
      "Omitting DodgerLoopDay, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... ECGFiveDays\n",
      "Omitting ECGFiveDays, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... UMD\n",
      "Omitting UMD, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Plane\n",
      "Omitting Plane, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... ECG200\n",
      "Omitting ECG200, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... GunPointAgeSpan\n",
      "Omitting GunPointAgeSpan, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... DiatomSizeReduction\n",
      "Omitting DiatomSizeReduction, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... MedicalImages\n",
      "Omitting MedicalImages, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Meat\n",
      "Omitting Meat, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Trace\n",
      "Omitting Trace, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... MiddlePhalanxOutlineAgeGroup\n",
      "Omitting MiddlePhalanxOutlineAgeGroup, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Chinatown\n",
      "Omitting Chinatown, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... DistalPhalanxOutlineAgeGroup\n",
      "Omitting DistalPhalanxOutlineAgeGroup, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... WormsTwoClass\n",
      "Omitting WormsTwoClass, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... DistalPhalanxOutlineCorrect\n",
      "Omitting DistalPhalanxOutlineCorrect, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Strawberry\n",
      "Omitting Strawberry, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... OliveOil\n",
      "Omitting OliveOil, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... SmoothSubspace\n",
      "Omitting SmoothSubspace, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... Fungi\n",
      "Omitting Fungi, explainers univariate SHAP:False LIME:False ANCHOR:True already calculated.\n",
      "Loading... ElectricDevices\n",
      "Loading model in tf format...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-03-18 11:16:27.783941: W tensorflow/c/c_api.cc:304] Operation '{name:'AssignVariableOp_103' id:2364 op device:{requested: '/device:CPU:0', assigned: ''} def:{{{node AssignVariableOp_103}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false, _device=\"/device:CPU:0\"](conv_lstm1d/bias/v_2, Identity_101)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "memory_info: {'0': '12002 MiB', '1': '12002 MiB'}\n",
      "method: docker , port: 9015 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '11393 MiB', '1': '12002 MiB'}\n",
      "method: docker , port: 9017 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '11393 MiB', '1': '11393 MiB'}\n",
      "method: docker , port: 9019 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '10784 MiB', '1': '11393 MiB'}\n",
      "method: docker , port: 9021 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '10784 MiB', '1': '10784 MiB'}\n",
      "method: docker , port: 9023 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '10174 MiB', '1': '10784 MiB'}\n",
      "method: docker , port: 9025 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '10174 MiB', '1': '10174 MiB'}\n",
      "method: docker , port: 9027 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '9565 MiB', '1': '10174 MiB'}\n",
      "method: docker , port: 9029 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '9565 MiB', '1': '9565 MiB'}\n",
      "method: docker , port: 9031 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '8956 MiB', '1': '9565 MiB'}\n",
      "method: docker , port: 9033 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '8956 MiB', '1': '8956 MiB'}\n",
      "method: docker , port: 9035 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '8345 MiB', '1': '8956 MiB'}\n",
      "method: docker , port: 9037 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '8345 MiB', '1': '8347 MiB'}\n",
      "method: docker , port: 9039 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '8345 MiB', '1': '7738 MiB'}\n",
      "method: docker , port: 9041 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '7736 MiB', '1': '7738 MiB'}\n",
      "method: docker , port: 9043 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '7736 MiB', '1': '7128 MiB'}\n",
      "method: docker , port: 9045 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '7126 MiB', '1': '7128 MiB'}\n",
      "method: docker , port: 9047 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '7126 MiB', '1': '6519 MiB'}\n",
      "method: docker , port: 9049 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '6517 MiB', '1': '6519 MiB'}\n",
      "method: docker , port: 9051 , gpu: 1 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "memory_info: {'0': '6517 MiB', '1': '5910 MiB'}\n",
      "method: docker , port: 9053 , gpu: 0 , model_name: ElectricDevices , test_data: (101, 96, 1)\n",
      "Updated used ports: [{'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9053, 'container_id': '41676f02bea4', 'pid': 3611280}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9051, 'container_id': '2fb59805072f', 'pid': 3606989}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9049, 'container_id': '5a780f976a4c', 'pid': 3602644}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9047, 'container_id': 'f9922edda794', 'pid': 3598208}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9045, 'container_id': '390e5d8eb94a', 'pid': 3593846}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9043, 'container_id': 'a1c277112282', 'pid': 3589610}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9041, 'container_id': '072e0dc712e4', 'pid': 3585282}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9039, 'container_id': '263ffcd43b73', 'pid': 3580900}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9037, 'container_id': 'fbba6328f8cc', 'pid': 3576817}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9035, 'container_id': 'cf3d176e62c3', 'pid': 3572796}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9033, 'container_id': 'f7c826d1ef09', 'pid': 3568758}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9031, 'container_id': '1cb836467c05', 'pid': 3564733}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9029, 'container_id': 'e64de33cc7c4', 'pid': 3560775}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9027, 'container_id': 'ca25588ee59a', 'pid': 3556943}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9025, 'container_id': '629a7748385f', 'pid': 3553146}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9023, 'container_id': '34187d21c3c6', 'pid': 3549576}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9021, 'container_id': '6619ad900904', 'pid': 3546204}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9019, 'container_id': '9bd5c9355ae8', 'pid': 3542789}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9017, 'container_id': 'b814326474e2', 'pid': 3539399}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9015, 'container_id': 'eeb5218f4443', 'pid': 3536044}]\n",
      "We have  20 available ports\n",
      "Processing... ElectricDevices with trainX (12477, 96, 1) and testX (4160, 96, 1) \n",
      "ANCHOR: ...\n",
      "Updated used ports: [{'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9053, 'container_id': '41676f02bea4', 'pid': 3611280}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9051, 'container_id': '2fb59805072f', 'pid': 3606989}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9049, 'container_id': '5a780f976a4c', 'pid': 3602644}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9047, 'container_id': 'f9922edda794', 'pid': 3598208}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9045, 'container_id': '390e5d8eb94a', 'pid': 3593846}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9043, 'container_id': 'a1c277112282', 'pid': 3589610}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9041, 'container_id': '072e0dc712e4', 'pid': 3585282}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9039, 'container_id': '263ffcd43b73', 'pid': 3580900}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9037, 'container_id': 'fbba6328f8cc', 'pid': 3576817}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9035, 'container_id': 'cf3d176e62c3', 'pid': 3572796}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9033, 'container_id': 'f7c826d1ef09', 'pid': 3568758}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9031, 'container_id': '1cb836467c05', 'pid': 3564733}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9029, 'container_id': 'e64de33cc7c4', 'pid': 3560775}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9027, 'container_id': 'ca25588ee59a', 'pid': 3556943}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9025, 'container_id': '629a7748385f', 'pid': 3553146}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9023, 'container_id': '34187d21c3c6', 'pid': 3549576}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9021, 'container_id': '6619ad900904', 'pid': 3546204}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9019, 'container_id': '9bd5c9355ae8', 'pid': 3542789}, {'model_name': 'ElectricDevices', 'gpu': '1', 'port': 9017, 'container_id': 'b814326474e2', 'pid': 3539399}, {'model_name': 'ElectricDevices', 'gpu': '0', 'port': 9015, 'container_id': 'eeb5218f4443', 'pid': 3536044}]\n",
      "Using 19 cores for parallel processing\n",
      "Train set (parallel)\n",
      "Prediction for 0 is 2\n",
      "Prediction for 2 is 6\n",
      "Prediction for 1 is 4\n",
      "2025-03-18 11:18:36 - 9029 : alive, observ. index: 7 , prediction shape: (100, 7) , up to 3 first values: [7.97058638e-32 9.99999166e-01 5.85321201e-21]\n",
      "2025-03-18 11:18:59 - 9049 : alive, observ. index: 17 , prediction shape: (100, 7) , up to 3 first values: [9.27490196e-10 9.00610485e-06 6.10793382e-02]\n",
      "2025-03-18 11:19:06 - 9039 : alive, observ. index: 12 , prediction shape: (100, 7) , up to 3 first values: [3.87400362e-10 2.32689215e-19 4.81766238e-10]\n",
      "2025-03-18 11:19:59 - 9043 : alive, observ. index: 14 , prediction shape: (100, 7) , up to 3 first values: [4.58349088e-11 7.84103626e-09 9.93196607e-01]\n",
      "2025-03-18 11:20:16 - 9017 : alive, observ. index: 1 , prediction shape: (100, 7) , up to 3 first values: [4.10562248e-15 8.62415479e-24 2.64996277e-13]\n",
      "2025-03-18 11:20:17 - 9049 : alive, observ. index: 17 , prediction shape: (100, 7) , up to 3 first values: [4.71782000e-05 4.01844271e-04 6.24610379e-07]\n",
      "2025-03-18 11:20:25 - 9033 : alive, observ. index: 9 , prediction shape: (100, 7) , up to 3 first values: [7.31275797e-01 1.48427461e-07 7.16118375e-04]\n",
      "2025-03-18 11:20:25 - 9033 : alive, observ. index: 9 , prediction shape: (100, 7) , up to 3 first values: [4.81615143e-06 5.53121859e-09 3.56680000e-06]\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2025-01-13T14:43:35.046102Z",
     "start_time": "2025-01-13T14:43:34.997414Z"
    }
   },
   "cell_type": "code",
   "source": "1",
   "id": "cd9f7425a837453b",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 1
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "# Wafer\n",
    "# 2024-11-10 23:33:20 - Explanation for sample 0 of Train: [{'index': 0, 'success': True, 'prediction': '1', 'rule': {'feature_1': ['>-0.74'], 'feature_4': ['<=0.73'], 'feature_11': ['>-0.74'], 'feature_42': ['<=0.73'], 'feature_110': ['>-0.74']}, 'confidence': 0.9565217391304348, 'coverage': 0.7197}]\n",
    "# 2024-11-12 11:12:59 - Explanation for sample 13 of Train: [{'index': 13, 'success': True, 'prediction': '1', 'rule': {'feature_1': ['>0.40'], 'feature_10': ['>0.40'], 'feature_106': ['>0.40']}, 'confidence': 0.9507829977628636, 'coverage': 0.5061}]\n"
   ],
   "id": "56f1083f56972fc6"
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "#### Multivariate",
   "id": "be54e87a3b039c55"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-12-11T15:16:07.472419Z",
     "start_time": "2024-12-11T15:16:07.396984Z"
    }
   },
   "cell_type": "code",
   "source": [
    "datasets_multi_not_list = [\n",
    "    \"FaceDetection\",\n",
    "    \"EthanolConcentration\",\n",
    "    \"HandMovementDirection\",\n",
    "    \"SelfRegulationSCP1\",\n",
    "    \"SelfRegulationSCP2\",\n",
    "]\n",
    "datasets_multi_minus_FaceDetection_EthanolConcentration_last_HandMovementDirection = [a for a in datasets_multi if\n",
    "                                                                                      a not in datasets_multi_not_list] + [\n",
    "                                                                                         \"HandMovementDirection\",\n",
    "                                                                                         \"SelfRegulationSCP1\",\n",
    "                                                                                         \"SelfRegulationSCP2\"]\n",
    "print(datasets_multi_minus_FaceDetection_EthanolConcentration_last_HandMovementDirection)"
   ],
   "id": "80966216644a3876",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ArticularyWordRecognition', 'Handwriting', 'BasicMotions', 'Heartbeat', 'SelfRegulationSCP2', 'NATOPS', 'ERing', 'FingerMovements', 'Epilepsy', 'Libras', 'AtrialFibrillation', 'PenDigits', 'Cricket', 'LSST', 'UWaveGestureLibrary', 'RacketSports', 'SelfRegulationSCP1', 'HandMovementDirection']\n"
     ]
    }
   ],
   "execution_count": 20
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-30T21:59:11.138205Z",
     "start_time": "2024-11-30T21:59:11.068843Z"
    }
   },
   "cell_type": "code",
   "source": "p",
   "id": "3be8ac177e27fd30",
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Libras'"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "execution_count": 65
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "ms_client = ModelServingClient(MODEL_MANAGER, base_port=12015)\n",
    "# if p != 'SelfRegulationSCP1':\n",
    "#     ms_client.terminate_all_containers(p)\n",
    "\n",
    "N = 12 * 2\n",
    "directory = 'multivariate'\n",
    "\n",
    "# for p in datasets_uni_minus_ChlorineConcentration:\n",
    "for p in [\"Libras\"] + datasets_multi_minus_FaceDetection_EthanolConcentration_last_HandMovementDirection:\n",
    "    print(f'Loading... {p}')\n",
    "\n",
    "    explain_tr_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}tr.pickle')\n",
    "    explain_ts_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}ts.pickle')\n",
    "    if os.path.exists(explain_tr_path) and os.path.exists(explain_ts_path):\n",
    "        if os.path.getsize(explain_tr_path) > 1024 and os.path.getsize(explain_ts_path) > 1024:\n",
    "            print(\n",
    "                f\"Omitting {p}, explainers {directory} SHAP:{shap_enabled} LIME:{lime_enabled} ANCHOR:{anchor_enabled} already calculated.\")\n",
    "            continue\n",
    "\n",
    "    model_path = os.path.join(data_path, 'ds', directory, p)\n",
    "\n",
    "    model_temp, _, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory),\n",
    "                                                                    load_model=True)\n",
    "    last_instance = trainX[-1]\n",
    "    last_instance_for_predict = np.expand_dims(last_instance, axis=0)\n",
    "\n",
    "    if p != 'Libras':\n",
    "        model_info, tf_model_server_ports = ms_client.load_models(\n",
    "            directory, p, data=last_instance_for_predict, count=N, grow_gpu_mem=\"true\", method=\"docker\", gpu=None,\n",
    "            num_samples=1_000, req_free_memory_gb=1)\n",
    "\n",
    "    ms_client.update_used_ports(p)\n",
    "    available_ports = list(ms_client.used_ports)\n",
    "    if len(available_ports) == 0:\n",
    "        raise Exception(\"No available ports\")\n",
    "    print(\"We have \", len(available_ports), \"available ports:\", available_ports)\n",
    "\n",
    "    sv_tr, sv_ts, lime_tr, lime_ts, anchor_tr, anchor_ts = process_dataset(\n",
    "        None, None, p, directory, model_path, model_temp, available_ports, trainX, trainy, testX, testy,\n",
    "        bg_size, window_len, stride, explanation_method, absshap, shap_enabled, lime_enabled, anchor_enabled,\n",
    "        timeout_duration=int(2 * 60.0 * 60), alt_timeout_duration=int(2 * 60.0 * 60), parallel=True)\n",
    "\n",
    "    _, score = evaluate_model(model_temp, testX, testy, batch_size, None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    print(f'Saving bundle... {p},  SHAP: {shap_enabled}, LIME: {lime_enabled}, ANCHOR: {anchor_enabled}')\n",
    "    save_bundle(None, trainX, trainy, testX, testy, anchor_tr, anchor_ts, p,\n",
    "                dir=os.path.join(data_path, 'ds', directory), explain_prefix=explain_prefix, only_explain=True)\n",
    "    summary.append([directory, p, score, 'OK'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model_temp, trainX, trainy, testX, testy, sv_tr, sv_ts, lime_tr, lime_ts\n",
    "    gc.collect()\n",
    "    ms_client.terminate_all_containers(p)"
   ],
   "id": "65f2ea932a1c51a2",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "pd.DataFrame(summary)",
   "id": "c851a0e16c906aa4",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "pd.DataFrame(summary, columns=['dataset', 'score', 'status']).to_csv(data_path + '/ds/summary-multi-1.csv',\n",
    "                                                                     index=False)"
   ],
   "id": "28271e69ffa3378c",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "77459fbbb71eef70"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8 for UCI v4",
   "language": "python",
   "name": "uci_38v4"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

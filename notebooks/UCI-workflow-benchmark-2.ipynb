{
 "cells": [
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-25T13:03:52.502162Z",
     "start_time": "2024-10-25T13:03:51.628617Z"
    }
   },
   "cell_type": "code",
   "source": "! wget https://raw.githubusercontent.com/vsubbian/WindowSHAP/main/windowshap.py -P utils/",
   "id": "2d5455d1f0d57e56",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--2024-10-25 13:03:52--  https://raw.githubusercontent.com/vsubbian/WindowSHAP/main/windowshap.py\r\n",
      "Resolving raw.githubusercontent.com (raw.githubusercontent.com)... 185.199.110.133, 185.199.111.133, 185.199.109.133, ...\r\n",
      "Connecting to raw.githubusercontent.com (raw.githubusercontent.com)|185.199.110.133|:443... connected.\r\n",
      "HTTP request sent, awaiting response... 200 OK\r\n",
      "Length: 22720 (22K) [text/plain]\r\n",
      "Saving to: ‘utils/windowshap.py’\r\n",
      "\r\n",
      "windowshap.py       100%[===================>]  22.19K  --.-KB/s    in 0.003s  \r\n",
      "\r\n",
      "2024-10-25 13:03:52 (6.82 MB/s) - ‘utils/windowshap.py’ saved [22720/22720]\r\n",
      "\r\n"
     ]
    }
   ],
   "execution_count": 36
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:19.623965Z",
     "start_time": "2024-10-30T18:08:19.607625Z"
    }
   },
   "cell_type": "code",
   "source": [
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "id": "f639611a350d3b1d",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:20.553751Z",
     "start_time": "2024-10-30T18:08:20.198394Z"
    }
   },
   "cell_type": "code",
   "source": "ls",
   "id": "f29eaa35a6033885",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "anchor_explanation_cache_checkpoint.pkl\r\n",
      "anchor_explanations_coverage.pkl\r\n",
      "auxiliary_mushrooms.py\r\n",
      "best_model_seq_contrastive.keras\r\n",
      "dice_explanation_cache_checkpoint.pkl\r\n",
      "dice_explanation_cache_checkpoint_test.pkl\r\n",
      "dice_explanations_coverage.pkl\r\n",
      "dummy_auc_palm_scale_100_window1.pkl\r\n",
      "dummy_auc_palm_scale_100_window2.pkl\r\n",
      "dummy_auc_palm_scale_100_window3.pkl\r\n",
      "dummy_auc_palm_scale_100_window4.pkl\r\n",
      "dummy_auc_palm_scale_100_window5.pkl\r\n",
      "dummy_auc_palm_scale_100_window6.pkl\r\n",
      "dummy_auc_palm_scale_100_window7.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "dummy_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[0m\u001B[01;34mextracted_data\u001B[0m/\r\n",
      "\u001B[01;34mfast\u001B[0m/\r\n",
      "formatted_table_image.png\r\n",
      "lime_auc_palm_scale_100_w0.pkl\r\n",
      "lime_auc_palm_scale_100_window1.pkl\r\n",
      "lime_auc_palm_scale_100_window2.pkl\r\n",
      "lime_auc_palm_scale_100_window3.pkl\r\n",
      "lime_auc_palm_scale_100_window4.pkl\r\n",
      "lime_auc_palm_scale_100_window5.pkl\r\n",
      "lime_explanation_cache_checkpoint.pkl\r\n",
      "lime_explanations.pkl\r\n",
      "lime_explanations_v02.pkl_window1\r\n",
      "lime_explanations_v11.pkl\r\n",
      "lime_explanations_v12_train.pkl\r\n",
      "lime_explanations_v13_train.pkl\r\n",
      "lime_explanations_v13_train.pkl_window1\r\n",
      "lime_explanations_v13_train.pkl_window2\r\n",
      "lime_explanations_v13_train.pkl_window3\r\n",
      "lime_explanations_v13_train.pkl_window4\r\n",
      "lime_explanations_v13_train.pkl_window5\r\n",
      "lime_perturbational_acc_loss_100.pkl\r\n",
      "lime_perturbational_acc_loss_100_v2.pkl\r\n",
      "lime_perturbational_acc_loss.pkl\r\n",
      "lime_prop_perturbation_acc.pkl\r\n",
      "lime_prop_perturbation_scale_100_acc_test_fin.pkl\r\n",
      "lime_prop_perturbation_scale_100_acc_train_fin.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "lime_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[01;34mmodels\u001B[0m/\r\n",
      "\u001B[01;34mMushroomDataset\u001B[0m/\r\n",
      "\u001B[01;34mmushroom-serialised\u001B[0m/\r\n",
      "\u001B[01;34m__pycache__\u001B[0m/\r\n",
      "\u001B[01;36mREADME.txt\u001B[0m@\r\n",
      "\u001B[01;34mresults\u001B[0m/\r\n",
      "shap_explanation_cache_checkpoint.pkl\r\n",
      "shap_perturbational_acc_loss_100.pkl\r\n",
      "shap_perturbational_acc_loss.pkl\r\n",
      "shap_prop_perturbation_acc.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_test_fin.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_test.pkl\r\n",
      "shap_prop_perturbation_scale_10_acc_train_fin.pkl\r\n",
      "\u001B[01;34mshared\u001B[0m/\r\n",
      "\u001B[01;34mslow\u001B[0m/\r\n",
      "\u001B[01;34mutils\u001B[0m/\r\n",
      "\u001B[01;34mXAI-ISI\u001B[0m/\r\n"
     ]
    }
   ],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:26.293915Z",
     "start_time": "2024-10-30T18:08:23.437980Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_DEVICE_ORDER\"] = \"PCI_BUS_ID\"\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\"  # 0 - first gpu, 1 - second, \"0,1\" - both gpu, first used\n",
    "\n",
    "import gc\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "# import io\n",
    "import pickle\n",
    "import shap\n",
    "import sys\n",
    "import tensorflow as tf\n",
    "import tensorflow.python.keras as keras  # import keras\n",
    "import warnings\n",
    "from aeon.datasets import load_classification\n",
    "from keras.layers import ConvLSTM1D\n",
    "from keras.utils import to_categorical\n",
    "from lime import lime_tabular\n",
    "from numpy import mean, std\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from tensorflow.keras import backend as K\n",
    "from tensorflow.python.framework import ops\n",
    "from tensorflow.python.keras.initializers import glorot_uniform\n",
    "from tensorflow.python.util import deprecation\n",
    "from typing import List, Tuple, Any\n",
    "from utils.windowshap import SlidingWindowSHAP\n",
    "\n",
    "print(tf.__version__)  # print(tf.keras.__version__)"
   ],
   "id": "817f63ac03c18d12",
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "2024-10-30 18:08:24.973556: I tensorflow/core/platform/cpu_feature_guard.cc:182] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n",
      "To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n",
      "2024-10-30 18:08:25.633488: W tensorflow/compiler/tf2tensorrt/utils/py_utils.cc:38] TF-TRT Warning: Could not find TensorRT\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.13.1\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:26.979097Z",
     "start_time": "2024-10-30T18:08:26.846367Z"
    }
   },
   "cell_type": "code",
   "source": [
    "tf.compat.v1.disable_v2_behavior()\n",
    "\n",
    "warnings.filterwarnings('ignore')\n",
    "ops.logging.set_verbosity(ops.logging.ERROR)\n",
    "tf.get_logger().setLevel('FATAL')\n",
    "tf.compat.v1.logging.set_verbosity(tf.compat.v1.logging.ERROR)\n",
    "deprecation._PRINT_DEPRECATION_WARNINGS = False\n",
    "os.environ['TF_ENABLE_DEPRECATION_WARNINGS'] = '0'\n",
    "# os.environ['CUDA_LAUNCH_BLOCKING'] = '1'\n",
    "# os.environ['TF_CPP_MIN_VLOG_LEVEL'] = '0'\n",
    "os.environ['TF_CPP_MIN_LOG_LEVEL'] = '3'\n",
    "\n",
    "gpus = tf.config.list_physical_devices(\"GPU\")\n",
    "print(gpus)\n",
    "gpu_to_limit = gpus[0]  # or 1, watch out for CUDA_VISIBLE_DEVICES\n",
    "print(gpu_to_limit)"
   ],
   "id": "22334058a3a1fe88",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From /home/jovyan/.conda/envs/uci_38/lib/python3.8/site-packages/tensorflow/python/compat/v2_compat.py:107: disable_resource_variables (from tensorflow.python.ops.variable_scope) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "non-resource variables are not supported in the long term\n",
      "[PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')]\n",
      "PhysicalDevice(name='/physical_device:GPU:0', device_type='GPU')\n"
     ]
    }
   ],
   "execution_count": 4
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:29.768506Z",
     "start_time": "2024-10-30T18:08:29.713057Z"
    }
   },
   "cell_type": "code",
   "source": [
    "gpu_mem_mb = 24564  # tyle mamy\n",
    "tf.config.set_logical_device_configuration(\n",
    "    gpu_to_limit, [tf.config.LogicalDeviceConfiguration(memory_limit=gpu_mem_mb // 2)]\n",
    ")\n",
    "\n",
    "print(\"Num GPUs Available: \", len(tf.config.list_physical_devices('GPU')))\n",
    "\n",
    "data_path = \"shared/UCI-Benchmark/\"\n",
    "batch_size = 64"
   ],
   "id": "98ee37ad-06fd-4b3b-a2d5-f79467354f3d",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Num GPUs Available:  1\n"
     ]
    }
   ],
   "execution_count": 5
  },
  {
   "cell_type": "markdown",
   "id": "4e5897d9-89ca-457d-b6bd-4f4ed1ac878d",
   "metadata": {},
   "source": [
    "# Real Data example"
   ]
  },
  {
   "cell_type": "code",
   "id": "a94117ee-3b54-4f54-88ee-8b995b1979de",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:31.438616Z",
     "start_time": "2024-10-30T18:08:31.369752Z"
    }
   },
   "source": [
    "# convlstm model\n",
    "def load_dataset_aeon(dsname):\n",
    "    X, y, meta_data = load_classification(dsname)\n",
    "    X = np.moveaxis(X, 1, 2)\n",
    "\n",
    "    le = LabelEncoder()\n",
    "    y = le.fit_transform(y)\n",
    "    trainX, testX, trainy, testy = train_test_split(X, y)\n",
    "\n",
    "    trainy = to_categorical(trainy)\n",
    "    testy = to_categorical(testy)\n",
    "    return trainX, trainy, testX, testy\n",
    "\n",
    "\n",
    "# summarize scores\n",
    "def summarize_results(scores):\n",
    "    print(scores)\n",
    "    m, s = mean(scores), std(scores)\n",
    "    print('Accuracy: %.3f%% (+/-%.3f)' % (m, s))\n",
    "\n",
    "\n",
    "# run an experiment\n",
    "def train_model(trainX, trainy, testX, testy):\n",
    "    return 1.0, keras.models.Sequential([])\n",
    "\n",
    "\n",
    "def train_lstmconv_model(trainX, trainy, testX, testy):\n",
    "    return 1.0, keras.models.Sequential([])\n",
    "\n",
    "\n",
    "def evaluate_model(dataset, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset_aeon(dataset)\n",
    "    #fill missing with zeros\n",
    "    trainX = np.nan_to_num(trainX, nan=0)\n",
    "    testX = np.nan_to_num(testX, nan=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    trainX = scaler.fit_transform(trainX.reshape(-1, trainX.shape[-1])).reshape(trainX.shape)\n",
    "    testX = scaler.transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score, model = train_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r + 1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return model, trainX, trainy, testX, testy, score\n",
    "\n",
    "\n",
    "# run an experiment\n",
    "def evaluate_lstmconv_model(dataset, repeats=1):\n",
    "    # load data\n",
    "    trainX, trainy, testX, testy = load_dataset_aeon(dataset)\n",
    "\n",
    "    #fill missing with zeros\n",
    "    trainX = np.nan_to_num(trainX, nan=0)\n",
    "    testX = np.nan_to_num(testX, nan=0)\n",
    "\n",
    "    scaler = StandardScaler()\n",
    "    trainX = scaler.fit_transform(trainX.reshape(-1, trainX.shape[-1])).reshape(trainX.shape)\n",
    "    testX = scaler.transform(testX.reshape(-1, testX.shape[-1])).reshape(testX.shape)\n",
    "    # repeat experiment\n",
    "    scores = list()\n",
    "    for r in range(repeats):\n",
    "        score, model = train_lstmconv_model(trainX, trainy, testX, testy)\n",
    "        score = score * 100.0\n",
    "        print('>#%d: %.3f' % (r + 1, score))\n",
    "        scores.append(score)\n",
    "    # summarize results\n",
    "    summarize_results(scores)\n",
    "    return model, trainX, trainy, testX, testy, score\n"
   ],
   "outputs": [],
   "execution_count": 6
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "## save & load bundle",
   "id": "501b87308df64f8c"
  },
  {
   "cell_type": "code",
   "id": "d050ffed-4f3a-4454-bd51-76c56d27626a",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:33.555841Z",
     "start_time": "2024-10-30T18:08:33.492559Z"
    }
   },
   "source": [
    "def save_bundle(model, trainX, trainy, testX, testy, svtr, svts, dsname,\n",
    "                dir='./results', explain_prefix=\"sv\", only_explain=False):\n",
    "    if not os.path.isdir(f'{dir}/{dsname}'):\n",
    "        os.makedirs(f'{dir}/{dsname}')\n",
    "\n",
    "    if not only_explain:\n",
    "        # model.save(f'{dir}/{dsname}/model.h5', save_format='h5')\n",
    "        model.save(f'{dir}/{dsname}/model_tf', save_format='tf')\n",
    "\n",
    "        pickle.dump(trainX, open(f'{dir}/{dsname}/trainX.pickle', 'wb'))\n",
    "        pickle.dump(trainy, open(f'{dir}/{dsname}/trainy.pickle', 'wb'))\n",
    "        pickle.dump(testX, open(f'{dir}/{dsname}/testX.pickle', 'wb'))\n",
    "        pickle.dump(testy, open(f'{dir}/{dsname}/testy.pickle', 'wb'))\n",
    "\n",
    "    pickle.dump(svtr, open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'wb'))\n",
    "    pickle.dump(svts, open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'wb'))\n",
    "\n",
    "\n",
    "def load_bundle(dsname, dir='./results', explain_prefix=\"sv\", only_explain=False):\n",
    "    model, trainX, trainy, testX, testy = None, None, None, None, None,\n",
    "    if not only_explain:\n",
    "        tf_model_path = f'{dir}/{dsname}/model_tf'\n",
    "        h5_model_path = f'{dir}/{dsname}/model.h5'\n",
    "\n",
    "        if os.path.exists(tf_model_path):\n",
    "            print(\"Loading model in tf format...\")\n",
    "            model = tf.keras.models.load_model(tf_model_path)\n",
    "        elif os.path.exists(h5_model_path):\n",
    "            print(\"Loading model in h5 format...\")\n",
    "            model = tf.keras.models.load_model(h5_model_path,\n",
    "                                               custom_objects={'GlorotUniform': glorot_uniform,\n",
    "                                                               'ConvLSTM1D': ConvLSTM1D})\n",
    "        else:\n",
    "            raise FileNotFoundError(\n",
    "                f'Nie znaleziono modelu w formatach SavedModel ani H5 dla {dsname} w katalogu {dir}.')\n",
    "        # print(model.summary())\n",
    "\n",
    "        trainX = pickle.load(open(f'{dir}/{dsname}/trainX.pickle', 'rb'))\n",
    "        trainy = pickle.load(open(f'{dir}/{dsname}/trainy.pickle', 'rb'))\n",
    "        testX = pickle.load(open(f'{dir}/{dsname}/testX.pickle', 'rb'))\n",
    "        testy = pickle.load(open(f'{dir}/{dsname}/testy.pickle', 'rb'))\n",
    "\n",
    "    svtr = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}tr.pickle', 'rb'))\n",
    "    svts = pickle.load(open(f'{dir}/{dsname}/{explain_prefix}ts.pickle', 'rb'))\n",
    "\n",
    "    return model, trainX, trainy, testX, testy, svtr, svts"
   ],
   "outputs": [],
   "execution_count": 7
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:35.077042Z",
     "start_time": "2024-10-30T18:08:35.017987Z"
    }
   },
   "cell_type": "code",
   "source": [
    "class HiddenPrints:\n",
    "    def __enter__(self):\n",
    "        self._original_stdout = sys.stdout\n",
    "        self._original_stderr = sys.stderr\n",
    "        sys.stdout = open(os.devnull, 'w')\n",
    "        sys.stderr = open(os.devnull, 'w')\n",
    "\n",
    "    def __exit__(self, exc_type, exc_val, exc_tb):\n",
    "        sys.stdout.close()\n",
    "        sys.stderr.close()\n",
    "        sys.stdout = self._original_stdout\n",
    "        sys.stderr = self._original_stderr\n",
    "\n",
    "\n",
    "# @tf.function\n",
    "def evaluate_model(model, testX, testy, batch_size, steps):\n",
    "    # f = io.StringIO()\n",
    "    # with redirect_stdout(f), redirect_stderr(f):\n",
    "    # sys.stderr = open(os.devnull, 'w')\n",
    "    if steps is not None:\n",
    "        result = model.evaluate(testX, testy, batch_size=batch_size, verbose=0, steps=steps)\n",
    "    else:\n",
    "        result = model.evaluate(testX, testy, batch_size=batch_size, verbose=0)\n",
    "    # sys.stderr = sys.__stderr__\n",
    "    return result"
   ],
   "id": "81d98f04cd5c1143",
   "outputs": [],
   "execution_count": 8
  },
  {
   "cell_type": "markdown",
   "id": "f885998e-9e52-4d0f-9398-df285e718c7f",
   "metadata": {},
   "source": "# Univariate"
  },
  {
   "cell_type": "code",
   "id": "5adfe00ef00318a3",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:38.574683Z",
     "start_time": "2024-10-30T18:08:38.511714Z"
    }
   },
   "source": [
    "#list directoies in  folder\n",
    "datasets_uni = [f.name for f in os.scandir(data_path + 'ds/univariate') if f.is_dir() and not f.name.startswith('.')]\n",
    "print(datasets_uni)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['DodgerLoopGame', 'ProximalPhalanxTW', 'DodgerLoopDay', 'ECGFiveDays', 'UMD', 'Plane', 'ECG200', 'TwoPatterns', 'GunPointAgeSpan', 'DiatomSizeReduction', 'UWaveGestureLibraryAll', 'MedicalImages', 'Meat', 'Trace', 'MiddlePhalanxOutlineAgeGroup', 'Chinatown', 'DistalPhalanxOutlineAgeGroup', 'WormsTwoClass', 'DistalPhalanxOutlineCorrect', 'Strawberry', 'OliveOil', 'UWaveGestureLibraryX', 'SmoothSubspace', 'Fungi', 'ElectricDevices', 'SwedishLeaf', 'CricketX', 'ECG5000', 'PhalangesOutlinesCorrect', 'FaceFour', 'SyntheticControl', 'FordA', 'PowerCons', 'BeetleFly', 'GunPointMaleVersusFemale', 'Yoga', 'Herring', 'Crop', 'RefrigerationDevices', 'Worms', 'OSULeaf', 'ItalyPowerDemand', 'GunPoint', 'CBF', 'Symbols', 'ToeSegmentation2', 'TwoLeadECG', 'SmallKitchenAppliances', 'ShapesAll', 'ScreenType', 'Computers', 'CricketY', 'SonyAIBORobotSurface2', 'Wafer', 'MiddlePhalanxOutlineCorrect', 'Lightning2', 'UWaveGestureLibraryY', 'Wine', 'ProximalPhalanxOutlineCorrect', 'ShapeletSim', 'CricketZ', 'BME', 'Beef', 'DodgerLoopWeekend', 'UWaveGestureLibraryZ', 'ProximalPhalanxOutlineAgeGroup', 'LargeKitchenAppliances', 'FordB', 'FreezerRegularTrain', 'Lightning7', 'BirdChicken', 'GunPointOldVersusYoung', 'Coffee', 'FreezerSmallTrain', 'SonyAIBORobotSurface1', 'Adiac', 'MiddlePhalanxTW', 'Earthquakes', 'DistalPhalanxTW', 'FiftyWords', 'WordSynonyms', 'MoteStrain', 'ChlorineConcentration', 'InsectWingbeatSound']\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {
    "scrolled": true,
    "ExecuteTime": {
     "end_time": "2024-10-25T10:46:40.618478Z",
     "start_time": "2024-10-25T10:46:38.479963Z"
    }
   },
   "cell_type": "code",
   "source": [
    "for p in datasets_uni:\n",
    "    print(f'Loading... {p}')\n",
    "    ##trainsvo is shap calculated for train, testsv is shap calculated for test (NOTE: these are absolute values of shap)\n",
    "    model, trainXo, trainyo, testX, testy, trainsvo, testsv = load_bundle(p, dir=data_path + 'ds/univariate')\n",
    "    # with HiddenPrints():\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size=batch_size, steps=None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    K.clear_session()\n",
    "    del model, trainXo, trainyo, testX, testy, trainsvo, testsv\n",
    "    gc.collect()\n",
    "\n",
    "    break\n"
   ],
   "id": "d510e5f8-96cb-49e7-8830-b6c389836845",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... DodgerLoopGame\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-25 10:46:38.798457: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_2/kernel/Assign' id:1710 op device:{requested: '', assigned: ''} def:{{{node dense_2/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_2/kernel, dense_2/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-25 10:46:38.995269: W tensorflow/c/c_api.cc:304] Operation '{name:'module_wrapper_2/conv_lstm1d/kernel/v/Assign' id:1943 op device:{requested: '', assigned: ''} def:{{{node module_wrapper_2/conv_lstm1d/kernel/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](module_wrapper_2/conv_lstm1d/kernel/v, module_wrapper_2/conv_lstm1d/kernel/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-25 10:46:39.174286: W tensorflow/c/c_api.cc:304] Operation '{name:'loss_1/mul' id:1852 op device:{requested: '', assigned: ''} def:{{{node loss_1/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss_1/mul/x, loss_1/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy: 87.500\n"
     ]
    }
   ],
   "execution_count": 32
  },
  {
   "cell_type": "markdown",
   "id": "e7b57e83-6cce-4893-88b6-a9a905e486d9",
   "metadata": {},
   "source": [
    "# Multivariate"
   ]
  },
  {
   "cell_type": "code",
   "id": "4a669e7be07298e9",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:08:49.017536Z",
     "start_time": "2024-10-30T18:08:48.972162Z"
    }
   },
   "source": [
    "#list directoies in  folder\n",
    "datasets_multi = [f.name for f in os.scandir(data_path + '/ds/multivariate') if\n",
    "                  f.is_dir() and not f.name.startswith('.')]"
   ],
   "outputs": [],
   "execution_count": 11
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:04:52.543394Z",
     "start_time": "2024-10-29T21:04:50.538177Z"
    }
   },
   "cell_type": "code",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... ArticularyWordRecognition\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-29 21:04:51.045931: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 5323 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:51:00.0, compute capability: 8.6\n",
      "2024-10-29 21:04:51.046743: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:1 with 12282 MB memory:  -> device: 1, name: NVIDIA RTX A5500, pci bus id: 0000:9c:00.0, compute capability: 8.6\n",
      "2024-10-29 21:04:51.059459: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-10-29 21:04:51.156898: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/kernel/Assign' id:683 op device:{requested: '', assigned: ''} def:{{{node dense_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/kernel, dense_1/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-29 21:04:51.336212: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/bias/v/Assign' id:959 op device:{requested: '', assigned: ''} def:{{{node dense_1/bias/v/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/bias/v, dense_1/bias/v/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-29 21:04:51.462762: W tensorflow/c/c_api.cc:304] Operation '{name:'loss/mul' id:811 op device:{requested: '', assigned: ''} def:{{{node loss/mul}} = Mul[T=DT_FLOAT, _has_manual_control_dependencies=true](loss/mul/x, loss/dense_1_loss/value)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-29 21:04:51.604158: I tensorflow/compiler/xla/stream_executor/cuda/cuda_dnn.cc:432] Loaded cuDNN version 8600\n",
      "2024-10-29 21:04:51.711300: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-29 21:04:51.712418: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-29 21:04:51.712447: W tensorflow/compiler/xla/stream_executor/gpu/asm_compiler.cc:109] Couldn't get ptxas version : FAILED_PRECONDITION: Couldn't get ptxas/nvlink version string: INTERNAL: Couldn't invoke ptxas --version\n",
      "2024-10-29 21:04:51.713689: I tensorflow/tsl/platform/default/subprocess.cc:304] Start cannot spawn child process: No such file or directory\n",
      "2024-10-29 21:04:51.713745: W tensorflow/compiler/xla/stream_executor/gpu/redzone_allocator.cc:318] INTERNAL: Failed to launch ptxas\n",
      "Relying on driver to perform ptx compilation. \n",
      "Modify $PATH to customize ptxas location.\n",
      "This message will be only logged once.\n",
      "2024-10-29 21:04:52.095209: I tensorflow/compiler/xla/stream_executor/cuda/cuda_blas.cc:606] TensorFloat-32 will be used for the matrix multiplication. This will only be logged once.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ">Accuracy: 96.528\n"
     ]
    }
   ],
   "execution_count": 7,
   "source": [
    "for p in datasets_multi:\n",
    "    print(f'Loading... {p}')\n",
    "    model, trainXo, trainyo, testX, testy, trainsvo, testsv = load_bundle(p, dir=data_path + 'ds/multivariate')\n",
    "    # steps = len(testX) // batch_size\n",
    "    # if len(testX) % batch_size != 0:\n",
    "    #     steps += 1\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size, None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    # K.clear_session()\n",
    "    # del model, trainXo, trainyo, testX, testy, trainsvo, testsv\n",
    "    # gc.collect()\n",
    "\n",
    "    break"
   ],
   "id": "aeb0de70-888c-4a53-b52c-5a63ec007d31"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:04:58.815344Z",
     "start_time": "2024-10-29T21:04:58.758856Z"
    }
   },
   "cell_type": "code",
   "source": [
    "dir(model)\n",
    "model.summary()"
   ],
   "id": "f189c686729b3294",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential\"\n",
      "_________________________________________________________________\n",
      " Layer (type)                Output Shape              Param #   \n",
      "=================================================================\n",
      " reshape (Reshape)           (None, 3, 48, 9)          0         \n",
      "                                                                 \n",
      " conv_lstm1d (ConvLSTM1D)    (None, 3, 48, 64)         168448    \n",
      "                                                                 \n",
      " conv_lstm1d_1 (ConvLSTM1D)  (None, 3, 48, 32)         110720    \n",
      "                                                                 \n",
      " dropout (Dropout)           (None, 3, 48, 32)         0         \n",
      "                                                                 \n",
      " embedding (Flatten)         (None, 4608)              0         \n",
      "                                                                 \n",
      " dense (Dense)               (None, 100)               460900    \n",
      "                                                                 \n",
      " dense_1 (Dense)             (None, 25)                2525      \n",
      "                                                                 \n",
      "=================================================================\n",
      "Total params: 742593 (2.83 MB)\n",
      "Trainable params: 742593 (2.83 MB)\n",
      "Non-trainable params: 0 (0.00 Byte)\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "execution_count": 8
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-29T21:05:00.915475Z",
     "start_time": "2024-10-29T21:05:00.870861Z"
    }
   },
   "cell_type": "code",
   "source": [
    "print(trainXo.shape)\n",
    "print(trainsvo.shape)"
   ],
   "id": "948d4e1474b94303",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(431, 144, 9)\n",
      "(431, 144, 9)\n"
     ]
    }
   ],
   "execution_count": 9
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Explainers",
   "id": "8c78d0713c78f09f"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:09:01.536182Z",
     "start_time": "2024-10-30T18:09:01.475306Z"
    }
   },
   "cell_type": "code",
   "source": [
    "def prepare_data(file_path: str) -> pd.DataFrame:\n",
    "    df = pd.read_csv(file_path, header=None, sep='|')\n",
    "    df = df[0].str.split(',', expand=True).dropna(axis=1)\n",
    "    df.columns = df.iloc[0]\n",
    "    return df[1:]\n",
    "\n",
    "\n",
    "def load_background_data(trainX: np.ndarray, bg_size: int) -> np.ndarray:\n",
    "    indexes = np.arange(len(trainX))\n",
    "    np.random.shuffle(indexes)\n",
    "    maxid = min(bg_size, len(trainX))\n",
    "    return trainX[indexes[:maxid]]\n",
    "\n",
    "\n",
    "def compute_window_shap(model: Any, trainX: np.ndarray, testX: np.ndarray, window_len: int, stride: int,\n",
    "                        bg_data: np.ndarray, output_shape: int) -> Tuple[np.ndarray, np.ndarray]:\n",
    "    sv_ts = np.zeros((len(testX), testX.shape[1], testX.shape[2]))\n",
    "    sv_tr = np.zeros((len(trainX), trainX.shape[1], trainX.shape[2]))\n",
    "\n",
    "    for i in range(len(testX)):\n",
    "        gtw = SlidingWindowSHAP(model, stride, window_len, bg_data, testX[i:i + 1], model_type='lstm')\n",
    "        sv_ts[i, :, :] = gtw.shap_values(num_output=output_shape)\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "        gtw = SlidingWindowSHAP(model, stride, window_len, bg_data, trainX[i:i + 1], model_type='lstm')\n",
    "        sv_tr[i, :, :] = gtw.shap_values(num_output=output_shape)\n",
    "\n",
    "    return sv_tr, sv_ts\n",
    "\n",
    "\n",
    "def compute_deep_shap(model: Any, trainX: np.ndarray, testX: np.ndarray, bg_data: np.ndarray, absshap: bool) -> Tuple[\n",
    "    np.ndarray, np.ndarray]:\n",
    "    explainer = shap.DeepExplainer(model, bg_data)\n",
    "    shap_values_ts = explainer.shap_values(testX, check_additivity=False)\n",
    "    shap_values_tr = explainer.shap_values(trainX, check_additivity=False)\n",
    "\n",
    "    if absshap:\n",
    "        return abs(np.array(shap_values_tr)).mean(axis=0), abs(np.array(shap_values_ts)).mean(axis=0)\n",
    "\n",
    "    indexer_ts = np.argmax(model.predict(testX), axis=1)\n",
    "    sv_ts = np.concatenate([shap_values_ts[indexer_ts[i]][i, :] for i in range(len(testX))])\n",
    "\n",
    "    indexer_tr = np.argmax(model.predict(trainX), axis=1)\n",
    "    sv_tr = np.concatenate([shap_values_tr[indexer_tr[i]][i, :] for i in range(len(trainX))])\n",
    "\n",
    "    return sv_tr, sv_ts\n",
    "\n",
    "\n",
    "# , num_features: int = 10\n",
    "def compute_deep_lime(model: Any, trainX: np.ndarray, testX: np.ndarray) -> Tuple[\n",
    "    np.ndarray, np.ndarray]:\n",
    "    n_timesteps, n_features = trainX.shape[1], trainX.shape[2]\n",
    "\n",
    "    trainX_reshaped = trainX.reshape(-1, n_timesteps * n_features)\n",
    "    testX_reshaped = testX.reshape(-1, n_timesteps * n_features)\n",
    "\n",
    "    nan_count_train_X = np.isnan(trainX_reshaped).sum()\n",
    "    nan_count_test_X = np.isnan(testX_reshaped).sum()\n",
    "    total_train_elements = trainX_reshaped.size\n",
    "    total_test_elements = testX_reshaped.size\n",
    "\n",
    "    if nan_count_train_X > 0 or nan_count_test_X > 0:\n",
    "        nan_percentage_train_X = (nan_count_train_X / total_train_elements) * 100\n",
    "        nan_percentage_test_X = (nan_count_test_X / total_test_elements) * 100\n",
    "        print(f\"Warning: Found NaN values. Train NaNs: {nan_count_train_X} ({nan_percentage_train_X:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_X} ({nan_percentage_test_X:.2f}%)\")\n",
    "\n",
    "        most_frequent_train_X = np.nanmedian(trainX_reshaped)\n",
    "        # most_frequent_test = np.nanmedian(testX_reshaped)\n",
    "\n",
    "        # Calculate the most frequent value across both samples\n",
    "        combined_data_X = np.concatenate((trainX_reshaped[~np.isnan(trainX_reshaped)],\n",
    "                                          testX_reshaped[~np.isnan(testX_reshaped)]))\n",
    "        most_frequent_value_X = np.bincount(combined_data_X.astype(int)).argmax()\n",
    "\n",
    "        print(\n",
    "            f\"Replacing NaN values with the most frequent value in train: {most_frequent_train_X}, while most frequent overall is  {most_frequent_value_X}\")\n",
    "\n",
    "        trainX_reshaped = np.where(np.isnan(trainX_reshaped), most_frequent_train_X, trainX_reshaped)\n",
    "        testX_reshaped = np.where(np.isnan(testX_reshaped), most_frequent_train_X, testX_reshaped)\n",
    "\n",
    "    train_predictions = model.predict(trainX.reshape(-1, n_timesteps, n_features))\n",
    "    test_predictions = model.predict(testX.reshape(-1, n_timesteps, n_features))\n",
    "\n",
    "    print(\"# of non-Nan predictions for train\", np.count_nonzero(~np.isnan(train_predictions)),\n",
    "          \"of\", train_predictions.size)\n",
    "    print(\"# of non-Nan predictions for test\", np.count_nonzero(~np.isnan(test_predictions)),\n",
    "          \"of\", test_predictions.size)\n",
    "\n",
    "    nan_threshold_y = 1.0  # percent\n",
    "    nan_count_train_y = np.isnan(train_predictions).sum()\n",
    "    nan_count_test_y = np.isnan(test_predictions).sum()\n",
    "\n",
    "    nan_percentage_train_y = (nan_count_train_y / train_predictions.size) * 100\n",
    "    nan_percentage_test_y = (nan_count_test_y / test_predictions.size) * 100\n",
    "\n",
    "    if nan_percentage_train_y > nan_threshold_y or nan_percentage_test_y > nan_threshold_y:\n",
    "        print(f\"Error: NaN percentage in predictions exceeded threshold. \"\n",
    "              f\"Train NaNs: {nan_count_train_y} ({nan_percentage_train_y:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_y} ({nan_percentage_test_y:.2f}%).\")\n",
    "        print(\"Stopping LIME calculations and returning empty results.\")\n",
    "\n",
    "        return np.empty((len(trainX), n_timesteps, n_features)), np.empty(\n",
    "            (len(testX), n_timesteps, n_features))  # return np.array([]), np.array([])\n",
    "    elif nan_percentage_train_y > 0.0 or nan_percentage_test_y > 0.0:\n",
    "        print(f\"Warning: There are NaN's in predictions. \"\n",
    "              f\"Train NaNs: {nan_count_train_y} ({nan_percentage_train_y:.2f}%), \"\n",
    "              f\"Test NaNs: {nan_count_test_y} ({nan_percentage_test_y:.2f}%).\")\n",
    "\n",
    "    most_frequent_value_y = np.nanmedian(\n",
    "        train_predictions)  # np.bincount(train_predictions[~np.isnan(train_predictions)].astype(int)).argmax()\n",
    "\n",
    "    def safe_predict_fn(x):\n",
    "        prediction = model.predict(x.reshape(-1, n_timesteps, n_features))\n",
    "\n",
    "        # Check for NaNs in the prediction\n",
    "        if np.isnan(prediction).sum() > 0:\n",
    "            nan_count_prediction = np.isnan(prediction).sum()\n",
    "            nan_percentage_prediction = (nan_count_prediction / prediction.size) * 100\n",
    "            # Replace NaNs in the prediction with a default value  \n",
    "            prediction = np.nan_to_num(prediction, nan=most_frequent_value_y)\n",
    "            print(f\"Warning: Prediction contains NaN values \"\n",
    "                  f\"({nan_count_prediction} NaNs, {nan_percentage_prediction:.2f}%). \"\n",
    "                  f\"Replacing NaN values in prediction with {most_frequent_value_y}.\")\n",
    "\n",
    "        return prediction\n",
    "\n",
    "    feature_names = [f'feature_{i}' for i in range(n_timesteps * n_features)]\n",
    "    class_names = [f'class_{i}' for i in range(model.output_shape[-1])]\n",
    "\n",
    "    explainer = lime_tabular.LimeTabularExplainer(\n",
    "        training_data=trainX_reshaped,\n",
    "        feature_names=feature_names,\n",
    "        class_names=class_names,\n",
    "        mode=\"classification\",\n",
    "        discretize_continuous=True\n",
    "    )\n",
    "\n",
    "    # sv_ts = np.zeros((len(testX), testX.shape[1], testX.shape[2]))\n",
    "    # sv_tr = np.zeros((len(trainX), trainX.shape[1], trainX.shape[2]))\n",
    "    sv_ts = []\n",
    "    sv_tr = []\n",
    "\n",
    "    for i in range(len(testX)):\n",
    "        # for i in tqdm(range(len(testX)), desc=\"Processing test data\"):\n",
    "        # print(f\"[{i}] testX: start\")\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=testX_reshaped[i],\n",
    "            predict_fn=safe_predict_fn,\n",
    "            num_features=n_timesteps * n_features\n",
    "        )\n",
    "        weights = np.array([feature[1] for feature in explanation.as_list()])\n",
    "        shaped_weights = weights.reshape((n_timesteps, n_features))\n",
    "        sv_ts.append(shaped_weights)\n",
    "\n",
    "    for i in range(len(trainX)):\n",
    "        # for i in tqdm(range(len(trainX)), desc=\"Processing train data\"):\n",
    "        explanation = explainer.explain_instance(\n",
    "            data_row=trainX_reshaped[i],\n",
    "            predict_fn=safe_predict_fn,  # lambda x: model.predict(x.reshape(-1, n_timesteps, n_features))\n",
    "            num_features=n_timesteps * n_features\n",
    "        )\n",
    "        weights = np.array([feature[1] for feature in explanation.as_list()])\n",
    "        shaped_weights = weights.reshape((n_timesteps, n_features))\n",
    "        sv_tr.append(shaped_weights)\n",
    "\n",
    "    return np.array(sv_tr), np.array(sv_ts)\n",
    "\n",
    "\n",
    "def process_dataset(p: str, model: Any, trainX: np.ndarray, trainy: np.ndarray, testX: np.ndarray, testy: np.ndarray,\n",
    "                    bg_size: int, window_len: int, stride: int, explanation_method: str, absshap: bool,\n",
    "                    shap_enabled: bool, lime_enabled: bool) -> Tuple[\n",
    "    List[Any], Tuple[np.ndarray], List[Any], Tuple[np.ndarray]]:\n",
    "    print(f\"Processing... {p}\")\n",
    "    bg_data = load_background_data(trainX, bg_size)\n",
    "\n",
    "    sv_tr, sv_ts = None, None\n",
    "    lime_tr, lime_ts = None, None\n",
    "\n",
    "    if shap_enabled:\n",
    "        if explanation_method == 'window':\n",
    "            print(\"SHAP: window\")\n",
    "            sv_tr, sv_ts = compute_window_shap(model, trainX, testX, window_len, stride, bg_data, trainy.shape[1])\n",
    "        else:\n",
    "            print(\"SHAP: deep\")\n",
    "            sv_tr, sv_ts = compute_deep_shap(model, trainX, testX, bg_data, absshap)\n",
    "\n",
    "    if lime_enabled:\n",
    "        print(\"LIME: deep\")\n",
    "        lime_tr, lime_ts = compute_deep_lime(model, trainX, testX)\n",
    "\n",
    "    return sv_tr, sv_ts, lime_tr, lime_ts"
   ],
   "id": "4ce8f52b5e6198dd",
   "outputs": [],
   "execution_count": 12
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T18:09:02.984680Z",
     "start_time": "2024-10-30T18:09:02.931677Z"
    }
   },
   "cell_type": "code",
   "source": [
    "bg_size = 1000\n",
    "window_len = 23\n",
    "stride = 10\n",
    "explanation_method = 'deep'  # or 'window' if needed for SHAP window-based explanations\n",
    "absshap = True\n",
    "shap_enabled = False  # SHAP disabled\n",
    "lime_enabled = True  # LIME enabled\n",
    "explain_prefix = \"lv\"\n",
    "\n",
    "print(data_path, batch_size, bg_size, window_len, stride, explanation_method, absshap, shap_enabled, lime_enabled,\n",
    "      explain_prefix)\n",
    "\n",
    "summary = []"
   ],
   "id": "156dd16bb2c88281",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "shared/UCI-Benchmark/ 64 1000 23 10 deep True False True lv\n"
     ]
    }
   ],
   "execution_count": 13
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "directory = 'univariate'\n",
    "\n",
    "for p in datasets_uni:\n",
    "    print(f'Loading... {p}')\n",
    "\n",
    "    explain_tr_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}tr.pickle')\n",
    "    explain_ts_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}ts.pickle')\n",
    "    if os.path.exists(explain_tr_path) and os.path.exists(explain_ts_path):\n",
    "        if os.path.getsize(explain_tr_path) > 1024 and os.path.getsize(explain_ts_path) > 1024:\n",
    "            print(f\"Omitting {p}, explainers {directory} SHAP:{shap_enabled} LIME:{lime_enabled} already calculated.\")\n",
    "            continue\n",
    "\n",
    "    model, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory))\n",
    "\n",
    "    sv_tr, sv_ts, lime_tr, lime_ts = process_dataset(p, model, trainX, trainy, testX, testy,\n",
    "                                                     bg_size, window_len, stride, explanation_method, absshap,\n",
    "                                                     shap_enabled, lime_enabled)\n",
    "\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size, None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % (score))\n",
    "\n",
    "    print(f'Saving bundle... {p},  SHAP: {shap_enabled}, LIME: {lime_enabled}')\n",
    "    save_bundle(model, trainX, trainy, testX, testy, lime_tr, lime_ts, p,\n",
    "                dir=os.path.join(data_path, 'ds', directory), explain_prefix=explain_prefix, only_explain=True)\n",
    "    summary.append([directory, p, score, 'OK'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model, trainX, trainy, testX, testy, sv_tr, sv_ts, lime_tr, lime_ts\n",
    "    gc.collect()\n",
    "\n",
    "    # break"
   ],
   "id": "3d4301f2ee96f337",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    },
    "ExecuteTime": {
     "start_time": "2024-10-30T18:09:13.270387Z"
    }
   },
   "cell_type": "code",
   "source": [
    "directory = 'multivariate'\n",
    "\n",
    "for p in datasets_multi:\n",
    "    print(f'Loading... {p}')\n",
    "\n",
    "    explain_tr_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}tr.pickle')\n",
    "    explain_ts_path = os.path.join(data_path, 'ds', directory, p, f'{explain_prefix}ts.pickle')\n",
    "    if os.path.exists(explain_tr_path) and os.path.exists(explain_ts_path):\n",
    "        if os.path.getsize(explain_tr_path) > 1024 and os.path.getsize(explain_ts_path) > 1024:\n",
    "            print(f\"Omitting {p}, explainers {directory} SHAP:{shap_enabled} LIME:{lime_enabled} already calculated.\")\n",
    "            continue\n",
    "\n",
    "    model, trainX, trainy, testX, testy, _, _ = load_bundle(p, dir=os.path.join(data_path, 'ds', directory))\n",
    "\n",
    "    sv_tr, sv_ts, lime_tr, lime_ts = process_dataset(p, model, trainX, trainy, testX, testy,\n",
    "                                                     bg_size, window_len, stride, explanation_method, absshap,\n",
    "                                                     shap_enabled, lime_enabled)\n",
    "\n",
    "    _, score = evaluate_model(model, testX, testy, batch_size, None)\n",
    "    score = score * 100.0\n",
    "    print('>Accuracy: %.3f' % score)\n",
    "\n",
    "    print(f'Saving bundle... {p}, SHAP: {shap_enabled}, LIME: {lime_enabled}')\n",
    "    save_bundle(model, trainX, trainy, testX, testy, lime_tr, lime_ts, p,\n",
    "                dir=os.path.join(data_path, 'ds', directory), explain_prefix=explain_prefix, only_explain=True)\n",
    "    summary.append([directory, p, score, 'OK'])\n",
    "\n",
    "    tf.keras.backend.clear_session()\n",
    "    del model, trainX, trainy, testX, testy, sv_tr, sv_ts, lime_tr, lime_ts\n",
    "    gc.collect()\n",
    "\n",
    "    # break"
   ],
   "id": "2ebfe1441c2fb438",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading... ArticularyWordRecognition\n",
      "Omitting ArticularyWordRecognition, explainers multivariate SHAP:False LIME:True already calculated.\n",
      "Loading... Handwriting\n",
      "Omitting Handwriting, explainers multivariate SHAP:False LIME:True already calculated.\n",
      "Loading... BasicMotions\n",
      "Omitting BasicMotions, explainers multivariate SHAP:False LIME:True already calculated.\n",
      "Loading... HandMovementDirection\n",
      "Omitting HandMovementDirection, explainers multivariate SHAP:False LIME:True already calculated.\n",
      "Loading... Heartbeat\n",
      "Omitting Heartbeat, explainers multivariate SHAP:False LIME:True already calculated.\n",
      "Loading... FaceDetection\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2024-10-30 18:09:13.673803: I tensorflow/core/common_runtime/gpu/gpu_device.cc:1639] Created device /job:localhost/replica:0/task:0/device:GPU:0 with 12282 MB memory:  -> device: 0, name: NVIDIA RTX A5500, pci bus id: 0000:9c:00.0, compute capability: 8.6\n",
      "2024-10-30 18:09:13.687022: I tensorflow/compiler/mlir/mlir_graph_optimization_pass.cc:375] MLIR V1 optimization pass is not enabled\n",
      "2024-10-30 18:09:13.721764: I tensorflow/compiler/xla/stream_executor/cuda/cuda_driver.cc:753] failed to allocate 11.99GiB (12878610432 bytes) from device: CUDA_ERROR_OUT_OF_MEMORY: out of memory\n",
      "2024-10-30 18:09:13.870115: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/kernel/Assign' id:683 op device:{requested: '', assigned: ''} def:{{{node dense_1/kernel/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/kernel, dense_1/kernel/Initializer/random_uniform)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n",
      "2024-10-30 18:09:14.043233: W tensorflow/c/c_api.cc:304] Operation '{name:'dense_1/kernel/m/Assign' id:890 op device:{requested: '', assigned: ''} def:{{{node dense_1/kernel/m/Assign}} = AssignVariableOp[_has_manual_control_dependencies=true, dtype=DT_FLOAT, validate_shape=false](dense_1/kernel/m, dense_1/kernel/m/Initializer/zeros)}}' was changed by setting attribute after it was run by a session. This mutation will have no effect, and will trigger an error in the future. Either don't modify nodes after running them or create a new session.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing... FaceDetection\n",
      "LIME: deep\n"
     ]
    }
   ],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T15:04:51.430619913Z",
     "start_time": "2024-10-27T15:19:59.464273Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# model\n",
    "# trainX.shape\n",
    "# trainy.shape"
   ],
   "id": "c35dfed33e5d4c2",
   "outputs": [],
   "execution_count": 72
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-27T15:20:48.811416Z",
     "start_time": "2024-10-27T15:20:48.757962Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# save_bundle(model, trainX, trainy, testX, testy, lime_tr, lime_ts, p,\n",
    "#             dir=data_path + '/ds/multivariate', explain_prefix=\"lv\", only_explain=True)"
   ],
   "id": "2c44c2ed93f3d08f",
   "outputs": [],
   "execution_count": 75
  },
  {
   "metadata": {
    "jupyter": {
     "is_executing": true
    }
   },
   "cell_type": "code",
   "source": "pd.DataFrame(summary)",
   "id": "595155042144fc07",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-10-30T15:11:35.299205Z",
     "start_time": "2024-10-30T15:11:35.253565Z"
    }
   },
   "cell_type": "code",
   "source": [
    "pd.DataFrame(summary, columns=['dataset', 'score', 'status']).to_csv(data_path + '/ds/summary-multi-1.csv',\n",
    "                                                                     index=False)"
   ],
   "id": "64bf7ef009279ff3",
   "outputs": [],
   "execution_count": 16
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": "",
   "id": "8de0b71004672f9c"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Py3.8 for UCI v3",
   "language": "python",
   "name": "uci_38"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
